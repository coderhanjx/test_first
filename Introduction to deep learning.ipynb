{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、快速上手\n",
    "\n",
    "本文将以训练 FashionMNIST 数据集为例，简单地介绍如何使用 OneFlow 完成深度学习中的常见任务。通过文章中的链接可以跳转到各个子任务的专题介绍。\n",
    "\n",
    "详细的介绍请阅读本文。让我们先从导入必要的库开始："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FlowVision](https://github.com/Oneflow-Inc/vision) 是与 OneFlow 配套的、专用于计算机视觉任务的工具库，包含诸多模型、数据增强方法、数据变换操作、数据集等。我们在这里导入并使用 FlowVision 提供的数据变换模块 `transforms` 和数据集模块 `datasets`。可以通过下面的命令安装："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\n",
      "Requirement already satisfied: flowvision==0.0.54 in /usr/local/miniconda3/lib/python3.7/site-packages (0.0.54)\r\n",
      "Requirement already satisfied: rich in /usr/local/miniconda3/lib/python3.7/site-packages (from flowvision==0.0.54) (12.5.1)\r\n",
      "Requirement already satisfied: tabulate in /usr/local/miniconda3/lib/python3.7/site-packages (from flowvision==0.0.54) (0.8.10)\r\n",
      "Requirement already satisfied: six in /usr/local/miniconda3/lib/python3.7/site-packages (from flowvision==0.0.54) (1.16.0)\r\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/miniconda3/lib/python3.7/site-packages (from rich->flowvision==0.0.54) (0.9.1)\r\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/miniconda3/lib/python3.7/site-packages (from rich->flowvision==0.0.54) (4.3.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/miniconda3/lib/python3.7/site-packages (from rich->flowvision==0.0.54) (2.10.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install flowvision==0.0.54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import oneflow as flow\n",
    "import oneflow.nn as nn\n",
    "from flowvision import transforms\n",
    "from flowvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置 batch size 以及运行设备："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=64\n",
    "DEVICE = \"cuda\"\n",
    "print(\"Using {} device\".format(DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据\n",
    "\n",
    "OneFlow 可以使用 [Dataset 与 DataLoader](https://oneflow.cloud/drill/#/project/private/code?id=f7ae0d7f33564b2add5b51fffe81c19e&workspaceId) 加载数据。\n",
    "\n",
    "[flowvision.datasets](https://flowvision.readthedocs.io/en/stable/flowvision.datasets.html) 模块中包含了不少真实的数据集(如 MNIST、CIFAR10、FashionMNIST)。\n",
    "\n",
    "我们通过 `flowvision.vision.datasets.FashionMNIST` 获取 FashionMNIST 的训练集和测试集数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#训练数据\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    # 变换图像数据为tensor\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True,\n",
    "    source_url=\"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/\",\n",
    "\n",
    ")\n",
    "#测试数据\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True,\n",
    "    source_url=\"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```text\n",
    "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
    "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data_fashion2/FashionMNIST/raw/train-images-idx3-ubyte.gz\n",
    "26422272/? [00:47<00:00, 679825.75it/s]\n",
    "Extracting data_fashion2/FashionMNIST/raw/train-images-idx3-ubyte.gz to data_fashion2/FashionMNIST/raw\n",
    "...\n",
    "```\n",
    "\n",
    "数据集下载并解压到 `./data` 目录下。\n",
    "\n",
    "利用 [oneflow.utils.data.DataLoader](https://oneflow.readthedocs.io/en/master/utils.html#oneflow.utils.data.DataLoader) 可以将 `dataset` 封装为迭代器，方便后续训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: oneflow.Size([64, 1, 28, 28])\n",
      "y.shape: oneflow.Size([64])\n"
     ]
    }
   ],
   "source": [
    "#flow.utils.data.DataLoader的用法在技术文档有详细说明：https://oneflow.readthedocs.io/en/master/utils.html\n",
    "# BATCH_SIZE表示样本数量，shuffle=True表示对数据进行洗牌\n",
    "train_dataloader = flow.utils.data.DataLoader(\n",
    "    training_data, BATCH_SIZE, shuffle=True\n",
    ")\n",
    "test_dataloader = flow.utils.data.DataLoader(\n",
    "    test_data, BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "for x, y in train_dataloader:\n",
    "    print(\"x.shape:\", x.shape)\n",
    "    print(\"y.shape:\", y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```text\n",
    "x.shape: flow.Size([64, 1, 28, 28])\n",
    "y.shape: flow.Size([64])\n",
    "```\n",
    "> [Dataset 与 Dataloader](https://oneflow.cloud/drill/#/project/private/code?id=f7ae0d7f33564b2add5b51fffe81c19e&workspaceId)\n",
    "\n",
    "## 搭建网络\n",
    "\n",
    "想要搭建网络，只需要实现一个继承自 `nn.Module` 的类就可以了。在它的 `__init__` 方法中定义神经网络的结构，在它的 `forward` 方法中指定前向传播的计算逻辑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#通过子类化 nn.Module定义我们的神经网络，用__init__方法初始化神经网络层,每一个 nn.Module子类用forward正向传播方法实现对输入数据的操作\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),#nn.Linear()设置网络中的全连接层,两个参数是对输入张量和输出张量的设置\n",
    "            nn.ReLU(),#将输入小于0的值幅值为0，输入大于0的值不变,技术文档地址：https://oneflow.readthedocs.io/en/master/nn.html\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x) #flatten()对多维数据进行降维\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "#创建一个实例（对象），把它放到DEVICE上\n",
    "model = NeuralNetwork().to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```text\n",
    "NeuralNetwork(\n",
    "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
    "  (linear_relu_stack): Sequential(\n",
    "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
    "    (3): ReLU()\n",
    "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "> [搭建神经网络](https://oneflow.cloud/drill/#/project/private/code?id=7e351fbd41fef653f26078e91915e76d&workspaceId)\n",
    "\n",
    "## 训练模型\n",
    "\n",
    "为了训练模型，我们需要损失函数 `loss_fn` 和优化器 `optimizer`，损失函数用于评价神经网络预测的结果与 label 的差距；`optimizer` 调整网络的参数，使得网络预测的结果越来越接近 label（标准答案），这里选用 [oneflow.optim.SGD](https://oneflow.readthedocs.io/en/master/optim.html?highlight=optim.SGD#oneflow.optim.SGD)。这一过程被称为反向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().to(DEVICE)#交叉熵损失函数\n",
    "optimizer = flow.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个 `train` 函数进行训练，完成前向传播、计算 loss、反向传播更新模型参数等工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iter, model, loss_fn, optimizer):\n",
    "    size = len(iter.dataset)\n",
    "    for batch, (x, y) in enumerate(iter):\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current = batch * BATCH_SIZE\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时，定义一个 `test` 函数，用于检验模型的精度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(iter, model, loss_fn):\n",
    "    size = len(iter.dataset)\n",
    "    num_batches = len(iter)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with flow.no_grad():\n",
    "        for x, y in iter:\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            \n",
    "            pred = model(x)\n",
    "            test_loss += loss_fn(pred, y)\n",
    "            bool_value = (pred.argmax(1).to(dtype=flow.int64)==y)\n",
    "            correct += float(bool_value.sum().numpy())\n",
    "    test_loss /= num_batches\n",
    "    print(\"test_loss\", test_loss, \"num_batches \", num_batches)\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}, Avg loss: {test_loss:>8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后可以开始训练，定义5轮 epoch，每训练完一个 epoch 都使用 `test` 来评估一下网络的精度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.294746  [    0/60000]\n",
      "loss: 2.283025  [ 6400/60000]\n",
      "loss: 2.256728  [12800/60000]\n",
      "loss: 2.256682  [19200/60000]\n",
      "loss: 2.241470  [25600/60000]\n",
      "loss: 2.224264  [32000/60000]\n",
      "loss: 2.204131  [38400/60000]\n",
      "loss: 2.197441  [44800/60000]\n",
      "loss: 2.154429  [51200/60000]\n",
      "loss: 2.137798  [57600/60000]\n",
      "test_loss tensor(2.1481, device='cuda:0', dtype=oneflow.float32) num_batches  157\n",
      "Test Error: \n",
      " Accuracy: 44.0, Avg loss: 2.148141\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.135089  [    0/60000]\n",
      "loss: 2.147892  [ 6400/60000]\n",
      "loss: 2.076566  [12800/60000]\n",
      "loss: 2.055215  [19200/60000]\n",
      "loss: 2.028604  [25600/60000]\n",
      "loss: 2.045242  [32000/60000]\n",
      "loss: 1.980073  [38400/60000]\n",
      "loss: 1.965189  [44800/60000]\n",
      "loss: 1.937590  [51200/60000]\n",
      "loss: 1.887024  [57600/60000]\n",
      "test_loss tensor(1.8685, device='cuda:0', dtype=oneflow.float32) num_batches  157\n",
      "Test Error: \n",
      " Accuracy: 56.0, Avg loss: 1.868529\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.835631  [    0/60000]\n",
      "loss: 1.830590  [ 6400/60000]\n",
      "loss: 1.800443  [12800/60000]\n",
      "loss: 1.695851  [19200/60000]\n",
      "loss: 1.619170  [25600/60000]\n",
      "loss: 1.723282  [32000/60000]\n",
      "loss: 1.586374  [38400/60000]\n",
      "loss: 1.628229  [44800/60000]\n",
      "loss: 1.569531  [51200/60000]\n",
      "loss: 1.364907  [57600/60000]\n",
      "test_loss tensor(1.5009, device='cuda:0', dtype=oneflow.float32) num_batches  157\n",
      "Test Error: \n",
      " Accuracy: 60.7, Avg loss: 1.500863\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.505974  [    0/60000]\n",
      "loss: 1.508212  [ 6400/60000]\n",
      "loss: 1.389402  [12800/60000]\n",
      "loss: 1.474996  [19200/60000]\n",
      "loss: 1.360023  [25600/60000]\n",
      "loss: 1.293832  [32000/60000]\n",
      "loss: 1.311789  [38400/60000]\n",
      "loss: 1.264558  [44800/60000]\n",
      "loss: 1.249137  [51200/60000]\n",
      "loss: 1.223612  [57600/60000]\n",
      "test_loss tensor(1.2446, device='cuda:0', dtype=oneflow.float32) num_batches  157\n",
      "Test Error: \n",
      " Accuracy: 63.8, Avg loss: 1.244644\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.237073  [    0/60000]\n",
      "loss: 1.153883  [ 6400/60000]\n",
      "loss: 1.231585  [12800/60000]\n",
      "loss: 1.177375  [19200/60000]\n",
      "loss: 1.035840  [25600/60000]\n",
      "loss: 1.359277  [32000/60000]\n",
      "loss: 1.121107  [38400/60000]\n",
      "loss: 1.160553  [44800/60000]\n",
      "loss: 0.975173  [51200/60000]\n",
      "loss: 1.182948  [57600/60000]\n",
      "test_loss tensor(1.0845, device='cuda:0', dtype=oneflow.float32) num_batches  157\n",
      "Test Error: \n",
      " Accuracy: 64.4, Avg loss: 1.084451\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```text\n",
    "Epoch 1\n",
    "-------------------------------\n",
    "loss: 2.152148  [    0/60000]\n",
    "loss: 2.140148  [ 6400/60000]\n",
    "loss: 2.147773  [12800/60000]\n",
    "loss: 2.088032  [19200/60000]\n",
    "loss: 2.074728  [25600/60000]\n",
    "loss: 2.034325  [32000/60000]\n",
    "loss: 1.994112  [38400/60000]\n",
    "loss: 1.984397  [44800/60000]\n",
    "loss: 1.918280  [51200/60000]\n",
    "loss: 1.884574  [57600/60000]\n",
    "test_loss tensor(1.9015, device='cuda:0', dtype=oneflow.float32) num_batches  157\n",
    "Test Error: \n",
    " Accuracy: 56.3, Avg loss: 1.901461\n",
    "Epoch 2\n",
    "-------------------------------\n",
    "loss: 1.914766  [    0/60000]\n",
    "loss: 1.817333  [ 6400/60000]\n",
    "loss: 1.835239  [12800/60000]\n",
    "...\n",
    "```\n",
    "> [自动求梯度](https://oneflow.cloud/drill/#/project/private/code?id=d05f6d1ec8e587456389b95270d3b7ac&workspaceId=d05f6d1ec8e587456389b95270d3b7ac)\n",
    "> [反向传播与 optimizer](https://oneflow.cloud/drill/#/project/private/code?id=1a377e1aa82311b64e70473ec34aba85&workspaceId)\n",
    "\n",
    "## 保存与加载模型\n",
    "\n",
    "调用 [oneflow.save](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.save#oneflow.save) 可以保存模型。保存的模型可以通过 [oneflow.load](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.load#oneflow.load) 加载，用于预测等工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "flow.save(model.state_dict(), \"./model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [模型的加载与保存](https://oneflow.cloud/drill/#/project/private/code?id=1ce7600d419d1a609678c547cf1a9856&workspaceId=1ce7600d419d1a609678c547cf1a9856)\n",
    "\n",
    "## 交流 QQ 群\n",
    "\n",
    "安装或使用过程遇到问题，欢迎入群与众多 OneFlow 爱好者共同讨论交流：\n",
    "\n",
    "加 QQ 群 **331883** 或扫描二维码\n",
    "\n",
    "![OneFlow 技术交流](./imgs/qq_group.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、Tensor 张量\n",
    "\n",
    "神经网络中的数据，都存放在 Tensor 中，Tensor 类似多维数组或者数学上的矩阵。OneFlow 提供了很多用于操作 Tensor 的算子，Tensor 与算子一起构成神经网络。\n",
    "\n",
    "Tensor 有别于普通的多维数组的地方是：除了可以运行在 CPU 上外，它还可以运行在 其它 AI 芯片（如 NVIDIA GPU）上，因此可以提高运算速度。此外，OneFlow 还为张量提供了 [自动求导](https://oneflow.cloud/drill/#/project/private/code?id=d05f6d1ec8e587456389b95270d3b7ac&workspaceId=d05f6d1ec8e587456389b95270d3b7ac\n",
    ") 的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oneflow as flow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建 Tensor\n",
    "有多种方法创建 Tensor，包括：\n",
    "\n",
    "- 直接从数据创建\n",
    "- 通过 Numpy 数组创建\n",
    "- 使用算子创建\n",
    "\n",
    "### 直接从数据创建\n",
    "可以直接从数据创建 Tensor："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=oneflow.int64)\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=oneflow.float32)\n"
     ]
    }
   ],
   "source": [
    "x1 = flow.tensor([[1, 2], [3, 4]])\n",
    "x2 = flow.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到创建的 `x1`、`x2` Tensor，它们的类型分别是 `int64` 和 `float32`。\n",
    "\n",
    "```text\n",
    "tensor([[1, 2],\n",
    "        [3, 4]], dtype=oneflow.int64)\n",
    "tensor([[1., 2.],\n",
    "        [3., 4.]], dtype=oneflow.float32)\n",
    "```\n",
    "\n",
    "### 通过 Numpy 数组创建\n",
    "\n",
    "Tensor 可以通过 Numpy 数组创建，只需要在创建 Tensor 对象时，将 Numpy 数组作为参数传递即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=oneflow.float64)\n",
      "tensor([[0.3094, 0.7261, 0.0591],\n",
      "        [0.8024, 0.0231, 0.3863]], dtype=oneflow.float64)\n"
     ]
    }
   ],
   "source": [
    "x3 = flow.tensor(np.ones((2,3)))\n",
    "x4 = flow.tensor(np.random.rand(2,3))#生成服从\"0~1\"均匀分布的随机数,范围是[0,1)\n",
    "print(x3)\n",
    "print(x4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "tensor([[1., 1., 1.],\n",
    "        [1., 1., 1.]], dtype=oneflow.float64)\n",
    "tensor([[0.6213, 0.6142, 0.1592],\n",
    "        [0.5539, 0.8453, 0.8576]], dtype=oneflow.float64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过算子创建\n",
    "\n",
    "OneFlow 中还提供了一些算子，可以通过它们创建 Tensor。比如 [ones](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.ones#oneflow.ones)、 [zeros](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.zeros#oneflow.zeros),、[eye](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.eye#oneflow.eye)，它们分别创建全为1的张量、全为0的张量和单位张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=oneflow.float32)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=oneflow.float32)\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]], dtype=oneflow.float32)\n"
     ]
    }
   ],
   "source": [
    "x5 = flow.ones(2, 3)\n",
    "x6 = flow.zeros(2, 3)\n",
    "x7 = flow.eye(3)\n",
    "print(x5)\n",
    "print(x6)\n",
    "print(x7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "tensor([[1., 1., 1.],\n",
    "        [1., 1., 1.]], dtype=oneflow.float32)\n",
    "tensor([[0., 0., 0.],\n",
    "        [0., 0., 0.]], dtype=oneflow.float32)\n",
    "tensor([[1., 0., 0.],\n",
    "        [0., 1., 0.],\n",
    "        [0., 0., 1.]], dtype=oneflow.float32)\n",
    "```\n",
    "\n",
    "[randn](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.randn#oneflow.randn) 方法可以创建随机化的张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4458, -1.1573, -1.8464],\n",
      "        [-0.0031,  0.4284,  0.8948]], dtype=oneflow.float32)\n"
     ]
    }
   ],
   "source": [
    "x8 = flow.randn(2,3)\n",
    "print(x8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Tensor` 与 `tensor` 的区别\n",
    "细心的用户会发现，OneFlow 中有 [oneflow.Tensor](https://oneflow.readthedocs.io/en/master/tensor.html?highlight=oneflow.Tensor#oneflow.Tensor) 和 [oneflow.tensor](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.Tensor#oneflow.tensor) 两个接口，它们都能用来创建张量。那么它们有什么区别呢？\n",
    "\n",
    "简单而言，大写的 `Tensor` 数据类型默认限定为 `float32`，而小写的 `tensor` 的数据类型可以随着创建时的数据改变。以下代码展示了两者这方面的区别："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], dtype=oneflow.float32)\n",
      "tensor([1, 2, 3], dtype=oneflow.int64)\n",
      "tensor([1., 2., 3.], dtype=oneflow.float32)\n"
     ]
    }
   ],
   "source": [
    "print(flow.Tensor([1, 2, 3]))\n",
    "print(flow.tensor([1, 2, 3]))\n",
    "print(flow.tensor([1.0, 2.0, 3.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据结果为：\n",
    "\n",
    "```text\n",
    "tensor([1., 2., 3.], dtype=oneflow.float32)\n",
    "tensor([1, 2, 3], dtype=oneflow.int64)\n",
    "tensor([1., 2., 3.], dtype=oneflow.float32)\n",
    "```\n",
    "\n",
    "此外，大写的 `Tensor` 可以在创建时不指定具体数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oneflow.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "x9 = flow.Tensor(2, 3)\n",
    "print(x9.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oneflow.Size([2, 3])  \n",
    "因此，如果在创建张量的同时不想指定数据，那么常常用 `oneflow.Tensor`，否则，应该使用 `oneflow.tensor`。\n",
    "\n",
    "## Tensor 的属性\n",
    "\n",
    "Tensor 的 `shape`、`dtype`、`device` 属性分别描述了 Tensor 的形状、数据类型和所在的设备类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oneflow.Size([1, 4])\n",
      "oneflow.float32\n",
      "cpu:0\n"
     ]
    }
   ],
   "source": [
    "x9 = flow.randn(1,4)\n",
    "print(x9.shape)\n",
    "print(x9.dtype)\n",
    "print(x9.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出结果分别展示了张量的形状、数据类型和所处的设备（第0号 CPU 上，之所以有编号，是因为 OneFlow 很方便自然地支持分布式，可参考 [Consistent Tensor](../parallelism/03_consistent_tensor.md)）\n",
    "\n",
    "```text\n",
    "oneflow.Size([1, 4])\n",
    "oneflow.float32\n",
    "cpu:0\n",
    "```\n",
    "\n",
    "可以通过 [reshape](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.reshape#oneflow.reshape) 方法改变 Tensor 的形状，用 [to](https://oneflow.readthedocs.io/en/master/tensor.html?highlight=Tensor.to#oneflow.Tensor.to) 方法改变 Tensor 的数据类型和所处设备：\n",
    "\n",
    "```\n",
    "x10 = x9.reshape(2, 2)\n",
    "x11 = x10.to(dtype=flow.int32, device=flow.device(\"cuda\"))\n",
    "print(x10.shape)\n",
    "print(x11.dtype, x11.device)\n",
    "```\n",
    "\n",
    "```text\n",
    "oneflow.Size([2, 2])\n",
    "oneflow.int32 cuda:0\n",
    "```\n",
    "\n",
    "## 操作 Tensor 的常见算子\n",
    "\n",
    "OneFlow 中提供了大量的算子，对 Tensor 进行操作，它们大多在 [oneflow](https://oneflow.readthedocs.io/en/master/oneflow.html)、[oneflow.Tensor](https://oneflow.readthedocs.io/en/master/tensor.html)、[oneflow.nn](https://oneflow.readthedocs.io/en/master/nn.html)、[oneflow.nn.functional](https://oneflow.readthedocs.io/en/master/functional.html)这几个名称空间下。\n",
    "\n",
    "OneFlow 中的 Tensor，与 Numpy 数组一样易用。比如，支持与 Numpy 类似的切片操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([1., 1., 1., 1.], dtype=oneflow.float32)\n",
      "First column:  tensor([1., 1., 1., 1.], dtype=oneflow.float32)\n",
      "Last column: tensor([1., 1., 1., 1.], dtype=oneflow.float32)\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]], dtype=oneflow.float32)\n"
     ]
    }
   ],
   "source": [
    "tensor = flow.ones(4, 4)\n",
    "print('First row: ',tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column:', tensor[..., -1])\n",
    "tensor[:,1] = 0 #将第1列元素设为0，列号从0开始\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "First row:  tensor([1., 1., 1., 1.], dtype=oneflow.float32)\n",
    "First column:  tensor([1., 1., 1., 1.], dtype=oneflow.float32)\n",
    "Last column: tensor([1., 1., 1., 1.], dtype=oneflow.float32)\n",
    "tensor([[1., 0., 1., 1.],\n",
    "        [1., 0., 1., 1.],\n",
    "        [1., 0., 1., 1.],\n",
    "        [1., 0., 1., 1.]], dtype=oneflow.float32)\n",
    "```\n",
    "\n",
    "此外，OneFlow 中还有很多其它操作，如算数相关操作的 [add](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.add#oneflow.add)、[sub](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.sub#oneflow.sub)、[mul](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.mul#oneflow.mul)、[div](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.div#oneflow.div)等；位置相关操作的 [scatter](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.scatter#oneflow.scatter)、[gather](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.gather#oneflow.gather)、[gather_nd](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.gather_nd#oneflow.gather_nd)等；以及激活函数、卷积等（[relu](https://oneflow.readthedocs.io/en/master/functional.html?highlight=oneflow.relu#oneflow.nn.functional.relu)、[conv2d](https://oneflow.readthedocs.io/en/master/functional.html?highlight=oneflow.conv2d#oneflow.nn.functional.conv2d)），点击它们的链接可以查看更详细的 API 说明，并找到更多的其它算子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、Dataset 与 DataLoader\n",
    "\n",
    "OneFlow 的 `Dataset` 与 `DataLoader` 的行为与 [PyTorch](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) 的是一致的，都是为了让数据集管理与模型训练解耦。\n",
    "\n",
    "`Dataset` 类用于定义如何读取数据。对于常见的计算机视觉数据集（如 FashionMNIST），可以直接使用 [FlowVision](https://github.com/Oneflow-Inc/vision) 库的 `datasets` 模块提供的数据集类，可以帮助我们自动下载并加载一些流行的数据集，这些类都间接继承了 `Dataset` 类。对于其他数据集，可以通过继承 `Dataset` 类来自定义数据集类。\n",
    "\n",
    "`DataLoader` 将 `Dataset` 封装为迭代器，方便训练时遍历并操作数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\n",
      "Requirement already satisfied: flowvision==0.0.54 in /usr/local/miniconda3/lib/python3.7/site-packages (0.0.54)\r\n",
      "Requirement already satisfied: rich in /usr/local/miniconda3/lib/python3.7/site-packages (from flowvision==0.0.54) (12.5.1)\r\n",
      "Requirement already satisfied: six in /usr/local/miniconda3/lib/python3.7/site-packages (from flowvision==0.0.54) (1.16.0)\r\n",
      "Requirement already satisfied: tabulate in /usr/local/miniconda3/lib/python3.7/site-packages (from flowvision==0.0.54) (0.8.10)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/miniconda3/lib/python3.7/site-packages (from rich->flowvision==0.0.54) (2.10.0)\r\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/miniconda3/lib/python3.7/site-packages (from rich->flowvision==0.0.54) (0.9.1)\r\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/miniconda3/lib/python3.7/site-packages (from rich->flowvision==0.0.54) (4.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install flowvision==0.0.54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import oneflow as flow\n",
    "import oneflow.nn as nn\n",
    "from oneflow.utils.data import Dataset\n",
    "from flowvision import datasets\n",
    "from flowvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面导入的 [flowvision.transforms](https://flowvision.readthedocs.io/en/stable/flowvision.transforms.html) 提供了一些对图像数据进行变换的操作（如 `ToTensor` 可以将 PIL 图像或 NumPy 数组转换为张量），可以在数据集类中直接使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 FlowVision 加载数据集\n",
    "\n",
    "以下的例子展示了如何使用 `flowvision.datasets` 加载 FashionMNIST 数据集。\n",
    "\n",
    "我们向 `FashionMNIST` 类传入以下参数：\n",
    "- `root`：数据集存放的路径\n",
    "- `train`： `True` 代表下载训练集、`False` 代表下载测试集\n",
    "- `download=True`： 如果 `root` 路径下数据集不存在，则从网络下载\n",
    "- `transforms`：指定的数据转换方式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    source_url=\"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/\",\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    source_url=\"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一次运行，会下载数据集，输出：\n",
    "\n",
    "```text\n",
    "Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/train-images-idx3-ubyte.gz\n",
    "Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n",
    "26422272/? [00:02<00:00, 8090800.72it/s]\n",
    "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
    "\n",
    "Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/train-labels-idx1-ubyte.gz\n",
    "Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
    "29696/? [00:00<00:00, 806948.09it/s]\n",
    "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
    "\n",
    "Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/t10k-images-idx3-ubyte.gz\n",
    "Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
    "4422656/? [00:00<00:00, 19237994.98it/s]\n",
    "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
    "\n",
    "Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/t10k-labels-idx1-ubyte.gz\n",
    "Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
    "6144/? [00:00<00:00, 152710.85it/s]\n",
    "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
    "```\n",
    "\n",
    "## 遍历数据\n",
    "\n",
    "`Dataset` 对象，可以像 `list` 一样，用下表索引，比如 `training_data[index]`。\n",
    "以下的例子，随机访问 `training_data` 中的9个图片，并显示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "from random import randint\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = randint(0, len(training_data))\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze().numpy(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fashionMNIST](./imgs/fashionMNIST.png)\n",
    "\n",
    "## 自定义 Dataset\n",
    "\n",
    "通过继承 [oneflow.utils.data.Dataset](https://oneflow.readthedocs.io/en/master/utils.html?highlight=oneflow.utils.data.Dataset#oneflow.utils.data.Dataset) 可以实现自定义 `Dataset`，自定义 `Dataset` 同样可以配合下一节介绍的 `Dataloader` 使用，简化数据处理的流程。\n",
    "\n",
    "以下的例子展示了如何实现一个自定义 `Dataset`，它的关键步骤是：\n",
    "\n",
    "- 继承 `oneflow.utils.data.Dataset`\n",
    "- 实现类的 `__len__` 方法，返回结果通常为该数据集中的样本数量\n",
    "- 实现类的 `__getitem__` 方法，它的返回值对应了用户（或框架）调用 `dataset_obj[idx]` 时得到的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1., 2.], dtype=float32), array([8.], dtype=float32))\n",
      "(array([2., 3.], dtype=float32), array([13.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class CustomDataset(Dataset):\n",
    "    raw_data_x = np.array([[1, 2], [2, 3], [4, 6], [3, 1]], dtype=np.float32)\n",
    "    raw_label = np.array([[8], [13], [26], [9]], dtype=np.float32)\n",
    "\n",
    "    def __init__(self, transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(raw_label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = CustomDataset.raw_data_x[idx]\n",
    "        label = CustomDataset.raw_label[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return x, label\n",
    "\n",
    "custom_dataset = CustomDataset()\n",
    "print(custom_dataset[0])\n",
    "print(custom_dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```text\n",
    "(array([1., 2.], dtype=float32), array([8.], dtype=float32))\n",
    "(array([2., 3.], dtype=float32), array([13.], dtype=float32))\n",
    "```\n",
    "\n",
    "## 使用 DataLoader\n",
    "\n",
    "利用 Dataset 可以一次获取到所有数据。但是在训练中，往往有其它的需求，如：一次读取 batch size 份数据；1轮 epoch 训练后，数据重新打乱（reshuffle）等。\n",
    "\n",
    "这时候，使用 `DataLoader` 即可。 `DataLoader` 可以将 `Dataset` 封装为迭代器，方便训练循环中获取数据。如以下例子：\n",
    "\n",
    "- `batch_size=64` ： 指定一次迭代返回的数据 batch size\n",
    "- `shuffle` ：是否要随机打乱数据的顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x:oneflow.Size([64, 1, 28, 28]), shape of label: oneflow.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from oneflow.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "#next() 返回迭代器的下一个项目，iter() 函数生成迭代器\n",
    "x, label = next(iter(train_dataloader))\n",
    "print(f\"shape of x:{x.shape}, shape of label: {label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "```text\n",
    "shape of x:oneflow.Size([64, 1, 28, 28]), shape of label: oneflow.Size([64])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2, dtype=oneflow.int64)\n"
     ]
    }
   ],
   "source": [
    "img = x[0].squeeze().numpy()\n",
    "label = label[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：（随机输出一张图片）\n",
    "\n",
    "![dataloader item](./imgs/dataloader_item.png)\n",
    "\n",
    "```text\n",
    "tensor(9, dtype=oneflow.int64)\n",
    "```\n",
    "\n",
    "自然我们也可以在训练的循环中，使用 `DataLoader` 迭代器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([64, 1, 28, 28]) oneflow.Size([64])\n",
      "oneflow.Size([32, 1, 28, 28]) oneflow.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for x, label in train_dataloader:\n",
    "    print(x.shape, label.shape)\n",
    "    # training..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、搭建神经网络\n",
    "\n",
    "​神经网络的各层，可以使用 [oneflow.nn](https://oneflow.readthedocs.io/en/master/nn.html) 名称空间下的 API 搭建，它提供了构建神经网络所需的常见 Module（如 [oneflow.nn.Conv2d](https://oneflow.readthedocs.io/en/master/nn.html?highlight=oneflow.nn.Conv2D#oneflow.nn.Conv2d)，[oneflow.nn.ReLU](https://oneflow.readthedocs.io/en/master/nn.html?highlight=oneflow.nn.ReLU#oneflow.nn.ReLU) 等等）。 用于搭建网络的所有 Module 类都继承自 [oneflow.nn.Module](https://oneflow.readthedocs.io/en/master/module.html#oneflow.nn.Module)，多个简单的 Module 可以组合在一起构成更复杂的 Module，用这种方式，用户可以轻松地搭建和管理复杂的神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oneflow as flow\n",
    "import oneflow.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义 Module 类\n",
    "\n",
    "`oneflow.nn` 下提供了常见的 Module 类，我们可以直接使用它们，或者在它们的基础上，通过自定义 Module 类搭建神经网络。搭建过程包括：\n",
    "\n",
    "- 写一个继承自 `oneflow.nn.Module` 的类\n",
    "- 实现类的 `__init__` 方法，在其中构建神经网络的结构\n",
    "- 实现类的 `forward` 方法，这个方法针对 Module 的输入进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "net = NeuralNetwork()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码，会输出刚刚搭建的 `NeuralNetwork` 网络的结构：\n",
    "\n",
    "```text\n",
    "NeuralNetwork(\n",
    "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
    "  (linear_relu_stack): Sequential(\n",
    "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
    "    (3): ReLU()\n",
    "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
    "    (5): ReLU()\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "接着，调用 `net` （注意：不推荐显式调用 `forward`）即可完成前向传播："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([3], dtype=oneflow.int64)\n"
     ]
    }
   ],
   "source": [
    "X = flow.ones(1, 28, 28)\n",
    "logits = net(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "会得到类似以下的输出结果：\n",
    "\n",
    "```text\n",
    "Predicted class: tensor([1], dtype=oneflow.int32)\n",
    "```\n",
    "\n",
    "以上从数据输入、到网络计算，最终推理输出的流程，如下图所示：\n",
    "\n",
    "![todo](./imgs/neural-network-layers.png)\n",
    "\n",
    "## `flow.nn.functional`\n",
    "\n",
    "除了 `oneflow.nn` 外，[oneflow.nn.functional](https://oneflow.readthedocs.io/en/master/functional.html) 名称空间下也提供了不少 API。它与 `oneflow.nn` 在功能上有一定的重叠。比如 [nn.functional.relu](https://oneflow.readthedocs.io/en/master/functional.html?highlight=relu#oneflow.nn.functional.relu) 与 [nn.ReLU](https://oneflow.readthedocs.io/en/master/nn.html?highlight=relu#oneflow.nn.ReLU) 都可用于神经网络做 activation 操作。\n",
    "\n",
    "两者的区别主要有：\n",
    "\n",
    "- `nn` 下的 API 是类，需要先构造实例化对象，再调用；`nn.functional` 下的 API 是作为函数直接调用\n",
    "- `nn` 下的类内部自己管理了网络参数；而 `nn.functional` 下的函数，需要我们自己定义参数，每次调用时手动传入\n",
    "\n",
    "实际上，OneFlow 提供的大部分 Module 是通过封装 `nn.functional` 下的方法得到的。`nn.functional` 提供了更加细粒度管理网络的可能。 \n",
    "\n",
    "以下的例子，使用 `nn.functional` 中的方法，构建与上文中 `NeuralNetwork` 类等价的 Module `FunctionalNeuralNetwork`，读者可以体会两者的异同："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([7], dtype=oneflow.int64)\n"
     ]
    }
   ],
   "source": [
    "class FunctionalNeuralNetwork(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(FunctionalNeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.weight1 = nn.Parameter(flow.randn(28*28, 512))\n",
    "        self.bias1 = nn.Parameter(flow.randn(512))\n",
    "\n",
    "        self.weight2 = nn.Parameter(flow.randn(512, 512))\n",
    "        self.bias2 = nn.Parameter(flow.randn(512))\n",
    "\n",
    "        self.weight3 = nn.Parameter(flow.randn(512, 10))\n",
    "        self.bias3 = nn.Parameter(flow.randn(10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(1, 28*28)\n",
    "        out = flow.matmul(x, self.weight1)#.matmul()是矩阵的乘法，矩阵的行和列对应相乘求和。\n",
    "        out = out + self.bias1\n",
    "        out = nn.functional.relu(out)\n",
    "\n",
    "        out = flow.matmul(out, self.weight2)\n",
    "        out = out + self.bias2\n",
    "        out = nn.functional.relu(out)\n",
    "\n",
    "        out = flow.matmul(out, self.weight3)\n",
    "        out = out + self.bias3\n",
    "        out = nn.functional.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "net = FunctionalNeuralNetwork()\n",
    "X = flow.ones(1, 28, 28)\n",
    "logits = net(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 容器\n",
    "\n",
    "比较以上 `NeuralNetwork` 与 `FunctionalNeuralNetwork` 实现的异同，可以发现 [nn.Sequential](https://oneflow.readthedocs.io/en/master/nn.html?highlight=nn.Sequential#oneflow.nn.Sequential) 对于简化代码起到了重要作用。\n",
    "\n",
    "`nn.Sequential` 是一种特殊容器，只要是继承自 `nn.Module` 的类都可以放置放置到其中。\n",
    "\n",
    "它的特殊之处在于：当 Sequential 进行前向传播时，Sequential 会自动地将容器中包含的各层“串联”起来。具体来说，会按照各层加入 Sequential 的顺序，自动地将上一层的输出，作为下一层的输入传递，直到得到整个 Module 的最后一层的输出。\n",
    "\n",
    "以下是不使用 Sequential 构建网络的例子（不推荐）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,20,5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(20,64,5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果使用 Sequential，则看起来是这样，会显得更简洁。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySeqModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MySeqModel, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了 Sequential 外，还有 `nn.Modulelist` 及 `nn.ModuleDict`，除了会自动注册参数到整个网络外，他们的其它行为类似 Python list、Python dict，只是常用简单的容器，不会自动进行前后层的前向传播，需要自己手工遍历完成各层的计算。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、Autograd\n",
    "\n",
    "神经网络的训练过程离不开 **反向传播算法**，在反向传播过程中，需要获取 loss 函数对模型参数的梯度，用于更新参数。\n",
    "\n",
    "OneFlow 提供了自动求导机制，可自动计算神经网络中参数的梯度。\n",
    "\n",
    "本文将先介绍计算图的基本概念，它有利于理解 OneFlow 自动求导的常见设置及限制，再介绍 OneFlow 中与自动求导有关的常见接口。\n",
    "\n",
    "## 计算图\n",
    "\n",
    "张量与算子，共同组成计算图，如以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oneflow as flow\n",
    "\n",
    "def loss(y_pred, y):\n",
    "    return flow.sum(1/2*(y_pred-y)**2)\n",
    "\n",
    "x = flow.ones(1, 5)  # 输入\n",
    "w = flow.randn(5, 3, requires_grad=True)#requires_grad说明当前量是否需要在计算中保留对应的梯度信息\n",
    "b = flow.randn(1, 3, requires_grad=True)\n",
    "z = flow.matmul(x, w) + b\n",
    "\n",
    "y = flow.zeros(1, 3)  # label\n",
    "l = loss(z,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它对应的计算图如下：\n",
    "\n",
    "![todo](./imgs/compute_graph.png)\n",
    "\n",
    "计算图中，像 `x`、`w`、`b`、`y` 这种只有输出，没有输入的节点称为 **叶子节点**；像 `loss` 这种只有输入没有输出的节点，称为 **根节点**。\n",
    "\n",
    "反向传播过程中，需要求得 `l` 对 `w`、`b` 的梯度，以更新这两个模型参数。因此，我们在创建它们时，设置 `requires_grad` 为 `True`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求梯度\n",
    "\n",
    "### backward 与梯度\n",
    "\n",
    "在反向传播的过程中，需要得到 `l` 分别对 `w`、`b` 的梯度 $\\frac{\\partial l}{\\partial w}$ 和 $\\frac{\\partial l}{\\partial b}$。我们只需要对 `l` 调用 `backward()` 方法，然后 OneFlow 就会自动计算梯度，并且存放到 `w` 与 `b` 的 `grad` 成员中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7841, -2.3939,  3.6692],\n",
      "        [-1.7841, -2.3939,  3.6692],\n",
      "        [-1.7841, -2.3939,  3.6692],\n",
      "        [-1.7841, -2.3939,  3.6692],\n",
      "        [-1.7841, -2.3939,  3.6692]], dtype=oneflow.float32)\n",
      "tensor([[-1.7841, -2.3939,  3.6692]], dtype=oneflow.float32)\n"
     ]
    }
   ],
   "source": [
    "l.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "tensor([[0.9397, 2.5428, 2.5377],\n",
    "        [0.9397, 2.5428, 2.5377],\n",
    "        [0.9397, 2.5428, 2.5377],\n",
    "        [0.9397, 2.5428, 2.5377],\n",
    "        [0.9397, 2.5428, 2.5377]], dtype=oneflow.float32)\n",
    "tensor([[0.9397, 2.5428, 2.5377]], dtype=oneflow.float32)\n",
    "```\n",
    "\n",
    "### 对非叶子节点求梯度\n",
    "默认情况下，只有 `requires_grad=True` 的叶子节点的梯度会被保留。非叶子节点的 `grad` 属性默认在 `backward` 执行过程中，会自动释放，不能查看。\n",
    "\n",
    "如果想保留并查看非叶子节点的梯度，可以调用 `Tensor.retain_grad` 方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-8.7423e-08, dtype=oneflow.float32)\n",
      "tensor(2., dtype=oneflow.float32)\n"
     ]
    }
   ],
   "source": [
    "from math import pi\n",
    "n1 = flow.tensor(pi/2, requires_grad=True)\n",
    "n2 = flow.sin(n1)\n",
    "n2.retain_grad()\n",
    "n3 = flow.pow(n2, 2)\n",
    "\n",
    "n3.backward()\n",
    "print(n1.grad)\n",
    "print(n2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码，既求 $\\frac{\\partial n_3}{\\partial n_1}$，也求 $\\frac{\\partial n_3}{\\partial n_2}$\n",
    "\n",
    "输出:\n",
    "\n",
    "```\n",
    "tensor(-8.7423e-08, dtype=oneflow.float32)\n",
    "tensor(2., dtype=oneflow.float32)\n",
    "```\n",
    "\n",
    "### 对一个计算图多次 `backward()`\n",
    "默认情况下，对于给定的计算图，只能调用 `backward()` 一次。比如，以下代码会报错："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "CheckFailedException",
     "evalue": "\n  File \"/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/api/python/autograd/autograd.cpp\", line 81, in Backward\n    one::GetThreadLocalAutogradEngine()->RunBackwardAndSaveGrads4LeafTensorIf( outputs, *gradients, retain_graph, create_graph)\n  File \"/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/core/autograd/autograd_engine.cpp\", line 456, in RunBackwardAndSaveGrads4LeafTensor\n    graph_task.Apply( true)\n  File \"/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/core/autograd/autograd_engine.cpp\", line 432, in Apply\n    node->Apply(create_graph_)\n  File \"/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/core/autograd/autograd_engine.cpp\", line 178, in Apply\n    Check failed: backward_fn_.get() != nullptr This FunctionNode with name `scalar_pow_backward` has been released.\nMaybe you try to backward through the node a second time. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCheckFailedException\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32/3078782464.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mn2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.7/site-packages/oneflow/framework/tensor.py\u001b[0m in \u001b[0;36m_backward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlazy_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         assert (\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.7/site-packages/oneflow/autograd/autograd.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(outputs, out_grads, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mconvert_to_tensor_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     )\n",
      "\u001b[0;31mCheckFailedException\u001b[0m: \n  File \"/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/api/python/autograd/autograd.cpp\", line 81, in Backward\n    one::GetThreadLocalAutogradEngine()->RunBackwardAndSaveGrads4LeafTensorIf( outputs, *gradients, retain_graph, create_graph)\n  File \"/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/core/autograd/autograd_engine.cpp\", line 456, in RunBackwardAndSaveGrads4LeafTensor\n    graph_task.Apply( true)\n  File \"/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/core/autograd/autograd_engine.cpp\", line 432, in Apply\n    node->Apply(create_graph_)\n  File \"/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/core/autograd/autograd_engine.cpp\", line 178, in Apply\n    Check failed: backward_fn_.get() != nullptr This FunctionNode with name `scalar_pow_backward` has been released.\nMaybe you try to backward through the node a second time. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."
     ]
    }
   ],
   "source": [
    "n1 = flow.tensor(10., requires_grad=True)\n",
    "n2 = flow.pow(n1, 2)\n",
    "n2.backward()\n",
    "n2.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "报错信息：\n",
    "\n",
    "> Maybe you try to backward through the node a second time. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.\n",
    "\n",
    "如果想要在同一个计算图上调用多次 `backward()`，需要在调用时设置 `retain_graph=True`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20., dtype=oneflow.float32)\n",
      "tensor(40., dtype=oneflow.float32)\n"
     ]
    }
   ],
   "source": [
    "n1 = flow.tensor(10., requires_grad=True)\n",
    "n2 = flow.pow(n1, 2)\n",
    "\n",
    "n2.backward(retain_graph=True)\n",
    "print(n1.grad)\n",
    "n2.backward()\n",
    "print(n1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```text\n",
    "tensor(20., dtype=oneflow.float32)\n",
    "tensor(40., dtype=oneflow.float32)\n",
    "```\n",
    "\n",
    "以上输出可知，OneFlow 会 **累加** 多次 `backward()` 计算得到的梯度。\n",
    "如果想清空梯度，可以调用 `zeros_` 方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20., dtype=oneflow.float32)\n",
      "tensor(20., dtype=oneflow.float32)\n"
     ]
    }
   ],
   "source": [
    "n1 = flow.tensor(10., requires_grad=True)\n",
    "n2 = flow.pow(n1, 2)\n",
    "\n",
    "n2.backward(retain_graph=True)\n",
    "print(n1.grad)\n",
    "n1.grad.zeros_()\n",
    "n2.backward()\n",
    "print(n1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```text\n",
    "tensor(20., dtype=oneflow.float32)\n",
    "tensor(20., dtype=oneflow.float32)\n",
    "```\n",
    "\n",
    "### 不记录某个 Tensor 的梯度\n",
    "\n",
    "默认情况下，OneFlow 会 tracing `requires_grad` 为 `True` 的 Tensor，自动求梯度。\n",
    "不过有些情况可能并不需要 OneFlow 这样做，比如只是想试一试前向推理。那么可以使用 [oneflow.no_grad](https://oneflow.readthedocs.io/en/master/oneflow.html#oneflow.no_grad) 或 [oneflow.Tensor.detach](https://oneflow.readthedocs.io/en/master/tensor.html#oneflow.Tensor.detach) 方法设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = flow.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with flow.no_grad():\n",
    "    z = flow.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```text\n",
    "True\n",
    "False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```text\n",
    "False\n",
    "```\n",
    "\n",
    "### 输出不是标量时如何求梯度\n",
    "通常，调用 `backward()` 方法的是神经网络的 loss，是一个标量。\n",
    "\n",
    "但是，如果不是标量，对 Tensor 调用 `backward()` 时会报错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "CheckFailedException",
     "evalue": "\n  File \"/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/api/python/autograd/autograd.cpp\", line 79, in Backward\n    CheckAndInitOutGrads(outputs, out_grads)\n  File \"/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/api/python/autograd/autograd.cpp\", line 61, in CheckAndInitOutGrads\n    Check failed: IsScalarTensor(*outputs.at(i)) Grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCheckFailedException\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32/3447902526.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.7/site-packages/oneflow/framework/tensor.py\u001b[0m in \u001b[0;36m_backward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlazy_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         assert (\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.7/site-packages/oneflow/autograd/autograd.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(outputs, out_grads, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mconvert_to_tensor_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     )\n",
      "\u001b[0;31mCheckFailedException\u001b[0m: \n  File \"/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/api/python/autograd/autograd.cpp\", line 79, in Backward\n    CheckAndInitOutGrads(outputs, out_grads)\n  File \"/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/api/python/autograd/autograd.cpp\", line 61, in CheckAndInitOutGrads\n    Check failed: IsScalarTensor(*outputs.at(i)) Grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "x = flow.randn(1, 2, requires_grad=True)\n",
    "y = 3*x + 1\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "报错信息：\n",
    "\n",
    "> Check failed: IsScalarTensor(*outputs.at(i)) Grad can be implicitly created only for scalar outputs\n",
    "\n",
    "而对 `y` 求 `sum` 后可以求梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.]], dtype=oneflow.float32)\n"
     ]
    }
   ],
   "source": [
    "x = flow.randn(1, 2, requires_grad=True)\n",
    "y = 3*x + 1\n",
    "y = y.sum()\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```text\n",
    "tensor([[3., 3.]], dtype=oneflow.float32)\n",
    "```\n",
    "\n",
    "错误原因及解决方法的分析请参考下文 “扩展阅读” 部分。\n",
    "\n",
    "## 扩展阅读\n",
    "\n",
    "`x` 张量中有两个元素，记作 $x_1$ 与 $x_2$，`y` 张量中的两个元素记作 $y_1$ 与 $y_2$，并且两者的关系是：\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = [x_1, x_2]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = [y_1, y_2] = [3x_1+1, 3x_2+1]\n",
    "$$\n",
    "\n",
    "此时，想直接求 $\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} =\n",
    " \\frac{[3x_1+1, 3x_2+1]}{[x_1, x_2]}\n",
    "$$\n",
    "\n",
    "在数学上是没有意义的，因此当然就报错了。\n",
    "实际上，当用户调用 `y.backward()` 时，其实想要的结果通常是：\n",
    "\n",
    "$$\n",
    "[\\frac{\\partial y_1}{\\partial x_1}, \\frac{\\partial y_2}{\\partial x_2}]\n",
    "$$\n",
    "\n",
    "当对 `y` 进行 `sum` 运算后：\n",
    "\n",
    "$$\n",
    "y = y_1 + y_2 = 3x_1 + 3x_2 + 2\n",
    "$$\n",
    "\n",
    "此时，调用 `backward()` 时，对 $x_1$ 和 $x_2$ 可求梯度：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_1} = \\frac{\\partial 3x_1 + 3x_2 + 2}{\\partial x_1} = 3\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_2} = \\frac{\\partial 3x_1 + 3x_2 + 2}{\\partial x_2} = 3\n",
    "$$\n",
    "\n",
    "除了使用 `sum` 之外，还可以使用更通用方法，即 **Vector Jacobian Product(VJP)** 完成非标量的根节点的梯度计算。依然用上文的例子，在反向传播过程中，OneFlow 会根据计算图生成雅可比矩阵：\n",
    "\n",
    "$$\n",
    "J = \\begin{pmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2}\\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2}\n",
    "\\end{pmatrix}\\\\\n",
    "= \\begin{pmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & 0 \\\\\n",
    "0                                 & \\frac{\\partial y_2}{\\partial x_2}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "只需提供一个与 $\\mathbf{y}$ 大小一致的向量 $\\mathbf{v}$，即可计算 VJP：\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "v_1\\\\\n",
    "v_2\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & 0 \\\\\n",
    "0                                 & \\frac{\\partial y_2}{\\partial x_2}\n",
    "\\end{pmatrix}=\n",
    "\\begin{bmatrix}\n",
    "v_1 \\frac{\\partial y_1}{\\partial x_1}\\\\\n",
    "v_2 \\frac{\\partial y_2}{\\partial x_2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "若向量 $\\mathbf{v}$ 是反向传播中上一层的梯度，VJP 的结果刚好是当前层要求的梯度。\n",
    "\n",
    "`backward` 方法是可以接受一个张量做参数的，该参数就是 VJP 中的 $\\mathbf{v}$，理解以上道理后，还可以使用以下的方式对张量求梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.]], dtype=oneflow.float32)\n"
     ]
    }
   ],
   "source": [
    "x = flow.randn(1, 2, requires_grad=True)\n",
    "y = 3*x + 1\n",
    "y.backward(flow.ones_like(y))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```text\n",
    "tensor([[3., 3.]], dtype=oneflow.float32)\n",
    "```\n",
    "\n",
    "**外部链接**\n",
    "\n",
    "- [Automatic Differentiation](http://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/slides/lec06.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、反向传播与 optimizer\n",
    "\n",
    "到目前为止，我们已经掌握如何使用 OneFlow [加载数据](https://oneflow.cloud/drill/#/project/private/code?id=f7ae0d7f33564b2add5b51fffe81c19e&workspaceId\n",
    ")、[搭建模型](https://oneflow.cloud/drill/#/project/private/code?id=7e351fbd41fef653f26078e91915e76d&workspaceId\n",
    ")、[自动计算模型参数的梯度](https://oneflow.cloud/drill/#/project/private/code?id=d05f6d1ec8e587456389b95270d3b7ac&workspaceId=d05f6d1ec8e587456389b95270d3b7ac\n",
    ")，将它们组合在一起，我们就可以利用反向传播算法训练模型。\n",
    "\n",
    "在 [oneflow.optim](https://oneflow.readthedocs.io/en/master/optim.html) 中，有各类 `optimizer`，它们可以简化实现反向传播的代码。\n",
    "\n",
    "本文将先介绍反向传播的基本概念，再介绍如何使用 `oneflow.optim` 类。\n",
    "\n",
    "## numpy 手工实现反向传播\n",
    "\n",
    "为了读者更方便理解反向传播与自动求导的关系，在这里提供了一份仅用 numpy 实现的简单模型的训练过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/500 loss:0.0034512376878410578\n",
      "100/500 loss:1.965487399502308e-06\n",
      "150/500 loss:1.05524122773204e-09\n",
      "200/500 loss:3.865352482534945e-12\n",
      "250/500 loss:3.865352482534945e-12\n",
      "300/500 loss:3.865352482534945e-12\n",
      "350/500 loss:3.865352482534945e-12\n",
      "400/500 loss:3.865352482534945e-12\n",
      "450/500 loss:3.865352482534945e-12\n",
      "500/500 loss:3.865352482534945e-12\n",
      "w:[[2.000001 ]\n",
      " [2.9999993]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ITER_COUNT = 500\n",
    "LR = 0.01\n",
    "\n",
    "# 前向传播\n",
    "def forward(x, w):\n",
    "    return np.matmul(x, w)\n",
    "\n",
    "\n",
    "# 损失函数\n",
    "def loss(y_pred, y):\n",
    "    return ((y_pred - y) ** 2).sum()\n",
    "\n",
    "\n",
    "# 计算梯度\n",
    "def gradient(x, y, y_pred):\n",
    "    return np.matmul(x.T, 2 * (y_pred - y))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 训练目标: Y = 2*X1 + 3*X2\n",
    "    x = np.array([[1, 2], [2, 3], [4, 6], [3, 1]], dtype=np.float32)\n",
    "    y = np.array([[8], [13], [26], [9]], dtype=np.float32)\n",
    "\n",
    "    w = np.array([[2], [1]], dtype=np.float32)\n",
    "    # 训练循环\n",
    "    for i in range(0, ITER_COUNT):\n",
    "        y_pred = forward(x, w)\n",
    "        l = loss(y_pred, y)\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"{i+1}/{500} loss:{l}\")\n",
    "\n",
    "        grad = gradient(x, y, y_pred)\n",
    "        w -= LR * grad\n",
    "\n",
    "    print(f\"w:{w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```text\n",
    "50/500 loss:0.0034512376878410578\n",
    "100/500 loss:1.965487399502308e-06\n",
    "150/500 loss:1.05524122773204e-09\n",
    "200/500 loss:3.865352482534945e-12\n",
    "250/500 loss:3.865352482534945e-12\n",
    "300/500 loss:3.865352482534945e-12\n",
    "350/500 loss:3.865352482534945e-12\n",
    "400/500 loss:3.865352482534945e-12\n",
    "450/500 loss:3.865352482534945e-12\n",
    "500/500 loss:3.865352482534945e-12\n",
    "w:[[2.000001 ]\n",
    " [2.9999993]]\n",
    "```\n",
    "\n",
    "注意我们选择的 loss 函数表达式为 $\\sum (y_{p} - y)^2$，因此 `loss` 对参数 `w`求梯度的代码为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y, y_pred):\n",
    "    return np.matmul(x.T, 2 * (y_pred - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新参数采用的是 [SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = gradient(x, y, y_pred)\n",
    "w -= LR*grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结而言，训练中的一次完整迭代包括以下步骤：\n",
    "\n",
    "1. 模型根据输入、参数，计算得出预测值 (`y_pred`)\n",
    "2. 计算 loss，即预测值与标签之间的误差\n",
    "3. 求 loss 对参数的梯度\n",
    "4. 更新参数\n",
    "\n",
    "其中 1-2 为前向传播过程；3-4为反向传播过程。\n",
    "\n",
    "## 超参 Hyperparameters\n",
    "\n",
    "超参数是有关模型训练设置的参数，可以影响到模型训练的效率和结果。如以上代码中的 `ITER_COUNT`、`LR` 就是超参数。\n",
    "\n",
    "## 使用 `oneflow.optim` 中的优化器类\n",
    "\n",
    "使用 `oneflow.optim` 中的优化器类进行反向传播会更简洁方便，接下来，我们展示如何使用。\n",
    "\n",
    "首先，先准备好数据和模型，使用 Module 的一个方便之处就是，可以把超参放置在 Module 中便于管理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oneflow as flow\n",
    "\n",
    "x = flow.tensor([[1, 2], [2, 3], [4, 6], [3, 1]], dtype=flow.float32)\n",
    "y = flow.tensor([[8], [13], [26], [9]], dtype=flow.float32)\n",
    "\n",
    "\n",
    "class MyLrModule(flow.nn.Module):\n",
    "    def __init__(self, lr, iter_count):\n",
    "        super().__init__()\n",
    "        self.w = flow.nn.Parameter(flow.tensor([[2], [1]], dtype=flow.float32))\n",
    "        self.lr = lr\n",
    "        self.iter_count = iter_count\n",
    "\n",
    "    def forward(self, x):\n",
    "        return flow.matmul(x, self.w)\n",
    "\n",
    "\n",
    "model = MyLrModule(0.01, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss 函数\n",
    "\n",
    "然后，选择好 loss 函数，OneFlow 自带了多种 loss 函数，我们在这里选择 [MSELoss](https://oneflow.readthedocs.io/en/master/nn.html?highlight=mseloss#oneflow.nn.MSELoss)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = flow.nn.MSELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造 optimizer\n",
    "反向传播的逻辑，都被封装在 optimizer 中。我们在此选择 [SGD](https://oneflow.readthedocs.io/en/master/optim.html?highlight=sgd#oneflow.optim.SGD)，你可以根据需要选择其它的优化算法，如 [Adam](https://oneflow.readthedocs.io/en/master/optim.html?highlight=adam#oneflow.optim.Adam)、[AdamW](https://oneflow.readthedocs.io/en/master/optim.html?highlight=adamw#oneflow.optim.AdamW) 等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = flow.optim.SGD(model.parameters(), model.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造 `optimizer`时，将模型参数及 learning rate 传递给 `SGD`。之后调用 `optimizer.step()`，在其内部就会自动完成对模型参数求梯度、并按照 SGD 算法更新模型参数。\n",
    "\n",
    "### 训练\n",
    "\n",
    "以上准备完成后，可以开始训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, model.iter_count):\n",
    "    y_pred = model(x)\n",
    "    l = loss(y_pred, y)\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"{i+1}/{model.iter_count} loss:{l.numpy()}\")\n",
    "\n",
    "    optimizer.zero_grad()#把梯度置零，也就是把loss关于weight的导数变成0\n",
    "    l.backward()\n",
    "    optimizer.step()#更新参数\n",
    "\n",
    "print(f\"\\nw: {model.w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "```text\n",
    "50/500 loss:0.003451163647696376\n",
    "100/500 loss:1.965773662959691e-06\n",
    "150/500 loss:1.103217073250562e-09\n",
    "200/500 loss:3.865352482534945e-12\n",
    "250/500 loss:3.865352482534945e-12\n",
    "300/500 loss:3.865352482534945e-12\n",
    "350/500 loss:3.865352482534945e-12\n",
    "400/500 loss:3.865352482534945e-12\n",
    "450/500 loss:3.865352482534945e-12\n",
    "500/500 loss:3.865352482534945e-12\n",
    "\n",
    "w: tensor([[2.],\n",
    "        [3.]], dtype=oneflow.float32, grad_fn=<accumulate_grad>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  七、模型的加载与保存\n",
    "\n",
    "对于模型的加载与保存，常用的场景有：\n",
    "\n",
    "- 将已经训练一段时间的模型保存，方便下次继续训练\n",
    "- 将训练好的模型保存，方便后续直接用于预测\n",
    "\n",
    "在本文中，我们将介绍，如何使用 [save](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.save#oneflow.save) 和 [load](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.load#oneflow.load) API 保存模型、加载模型。\n",
    "\n",
    "同时也会展示，如何加载预训练模型，完成预测任务。\n",
    "\n",
    "## 模型参数的获取与加载\n",
    "OneFlow 预先提供的各种 `Module` 或者用户自定义的 `Module`，都提供了 `state_dict` 方法获取模型所有的参数，它是以 “参数名-参数值” 形式存放的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.2925,  0.4499],\n",
      "        [ 0.3857, -0.6223],\n",
      "        [-0.0113,  0.3628]], dtype=oneflow.float32, requires_grad=True)), ('bias', tensor([-0.6041, -0.6079,  0.5429], dtype=oneflow.float32, requires_grad=True))])\n"
     ]
    }
   ],
   "source": [
    "import oneflow as flow\n",
    "m = flow.nn.Linear(2,3)\n",
    "print(m.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码，将显式构造好的 Linear Module 对象 m 中的参数打印出来：\n",
    "\n",
    "```text\n",
    "OrderedDict([('weight',\n",
    "              tensor([[-0.4297, -0.3571],\n",
    "                      [ 0.6797, -0.5295],\n",
    "                      [ 0.4918, -0.3039]], dtype=oneflow.float32, requires_grad=True)),\n",
    "             ('bias',\n",
    "              tensor([ 0.0977,  0.1219, -0.5372], dtype=oneflow.float32, requires_grad=True))])\n",
    "```\n",
    "\n",
    "通过调用 `Module` 的 `load_state_dict` 方法，可以加载参数，如以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], dtype=oneflow.float32, requires_grad=True)), ('bias', tensor([0., 0., 0.], dtype=oneflow.float32, requires_grad=True))])\n"
     ]
    }
   ],
   "source": [
    "myparams = {\"weight\":flow.ones(3,2), \"bias\":flow.zeros(3)}\n",
    "m.load_state_dict(myparams)\n",
    "print(m.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，我们自己构造的字典中的张量，已经被加载到 m Module 中：\n",
    "\n",
    "```text\n",
    "OrderedDict([('weight',\n",
    "              tensor([[1., 1.],\n",
    "                      [1., 1.],\n",
    "                      [1., 1.]], dtype=oneflow.float32, requires_grad=True)),\n",
    "             ('bias',\n",
    "              tensor([0., 0., 0.], dtype=oneflow.float32, requires_grad=True))])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型保存\n",
    "我们可以使用 [oneflow.save](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.save#oneflow.save)方法保存模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.save(m.state_dict(), \"./model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它的第一个参数的 Module 的参数，第二个是保存路径。以上代码，将 `m` Module 对象的参数，保存到了 `./model` 目录下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型加载\n",
    "\n",
    "使用 [oneflow.load](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.load#oneflow.load) 可以将参数从指定的磁盘路径加载参数到内存，得到存有参数的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = flow.load(\"./model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，再借助上文介绍的 `load_state_dict` 方法，就可以将字典加载到模型中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], dtype=oneflow.float32, requires_grad=True)), ('bias', tensor([0., 0., 0.], dtype=oneflow.float32, requires_grad=True))])\n"
     ]
    }
   ],
   "source": [
    "m2 = flow.nn.Linear(2,3)\n",
    "m2.load_state_dict(params)\n",
    "print(m2.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码，新构建了一个 Linear Module 对象 `m2`，并且将从上文保存得到的的参数加载到 `m2` 上。得到输出：\n",
    "\n",
    "```text\n",
    "OrderedDict([('weight', tensor([[1., 1.],\n",
    "        [1., 1.],\n",
    "        [1., 1.]], dtype=oneflow.float32, requires_grad=True)), ('bias', tensor([0., 0., 0.], dtype=oneflow.float32, requires_grad=True))])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用预训练模型进行预测\n",
    "\n",
    "OneFlow 是可以直接加载 PyTorch 的预训练模型，用于预测的。\n",
    "只要模型的作者能够确保搭建的模型的结构、参数名与 PyTorch 模型对齐。\n",
    "\n",
    "相关的例子可以在 [OneFlow Models 仓库的这个 README](https://github.com/Oneflow-Inc/models/tree/main/shufflenetv2#convert-pretrained-model-from-pytorch-to-oneflow) 查看。\n",
    "\n",
    "以下命令行，可以体验如何使用预训练好的模型，进行预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'models'...\n",
      "fatal: unable to access 'https://github.com/Oneflow-Inc/models.git/': GnuTLS recv error (-110): The TLS connection was non-properly terminated.\n",
      "bash: line 2: cd: models/shufflenetv2: No such file or directory\n",
      "bash: infer.sh: No such file or directory\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'git clone https://github.com/Oneflow-Inc/models.git\\ncd models/shufflenetv2\\nbash infer.sh\\n'' returned non-zero exit status 127.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32/2656990573.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'git clone https://github.com/Oneflow-Inc/models.git\\ncd models/shufflenetv2\\nbash infer.sh\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2401\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2402\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2403\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2404\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.7/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'git clone https://github.com/Oneflow-Inc/models.git\\ncd models/shufflenetv2\\nbash infer.sh\\n'' returned non-zero exit status 127."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git clone https://github.com/Oneflow-Inc/models.git\n",
    "cd models/shufflenetv2\n",
    "bash infer.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 八、静态图模块 nn.Graph\n",
    "\n",
    "目前，深度学习框架中模型的运行方式主要有两种，即 **动态图** 与 **静态图**，在 OneFlow 中，也被习惯称为 **Eager 模式** 和 **Graph 模式** 。\n",
    "\n",
    "这两种方式各有优缺点，OneFlow 对两种方式均提供了支持，默认情况下是 Eager 模式。如果你是按顺序阅读本基础专题的教程，那么，到目前为止所接触的所有代码都是 Eager 模式的代码。\n",
    "\n",
    "一般而言，动态图更易用，静态图性能更具优势。OneFlow 提供的 [nn.Graph](https://oneflow.readthedocs.io/en/master/graph.html) 模块，让用户可以用类似 Eager 的编程习惯，构建静态图并训练模型。\n",
    "\n",
    "## OneFlow 的 Eager 模式\n",
    "\n",
    "OneFlow 默认以 Eager 模式运行。\n",
    "\n",
    "因为要用到[flowvision](https://github.com/Oneflow-Inc/vision/tree/main/flowvision)，先用以下命令安装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\n",
      "Requirement already satisfied: flowvision in /usr/local/miniconda3/lib/python3.7/site-packages (0.0.54)\r\n",
      "Requirement already satisfied: tabulate in /usr/local/miniconda3/lib/python3.7/site-packages (from flowvision) (0.8.10)\r\n",
      "Requirement already satisfied: rich in /usr/local/miniconda3/lib/python3.7/site-packages (from flowvision) (12.5.1)\r\n",
      "Requirement already satisfied: six in /usr/local/miniconda3/lib/python3.7/site-packages (from flowvision) (1.16.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/miniconda3/lib/python3.7/site-packages (from rich->flowvision) (2.10.0)\r\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/miniconda3/lib/python3.7/site-packages (from rich->flowvision) (0.9.1)\r\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/miniconda3/lib/python3.7/site-packages (from rich->flowvision) (4.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install flowvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，使用 CIFAR10 数据集训练 `mobilenet_v2` 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Files already downloaded and verified\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 6.869030  [    0/50000]\n",
      "loss: 6.748667  [  320/50000]\n",
      "loss: 6.640531  [  640/50000]\n",
      "loss: 6.575492  [  960/50000]\n",
      "loss: 6.445483  [ 1280/50000]\n",
      "loss: 6.356844  [ 1600/50000]\n",
      "loss: 6.278995  [ 1920/50000]\n",
      "loss: 6.158844  [ 2240/50000]\n",
      "loss: 6.073194  [ 2560/50000]\n",
      "loss: 5.958980  [ 2880/50000]\n",
      "loss: 5.880669  [ 3200/50000]\n",
      "loss: 5.805188  [ 3520/50000]\n",
      "loss: 5.708918  [ 3840/50000]\n",
      "loss: 5.659461  [ 4160/50000]\n",
      "loss: 5.509037  [ 4480/50000]\n",
      "loss: 5.410415  [ 4800/50000]\n",
      "loss: 5.268206  [ 5120/50000]\n",
      "loss: 5.215643  [ 5440/50000]\n",
      "loss: 5.099469  [ 5760/50000]\n",
      "loss: 5.115852  [ 6080/50000]\n",
      "loss: 4.922927  [ 6400/50000]\n",
      "loss: 4.865592  [ 6720/50000]\n",
      "loss: 4.812086  [ 7040/50000]\n",
      "loss: 4.701884  [ 7360/50000]\n",
      "loss: 4.649385  [ 7680/50000]\n",
      "loss: 4.504488  [ 8000/50000]\n",
      "loss: 4.467904  [ 8320/50000]\n",
      "loss: 4.368468  [ 8640/50000]\n",
      "loss: 4.344764  [ 8960/50000]\n",
      "loss: 4.261994  [ 9280/50000]\n",
      "loss: 4.201000  [ 9600/50000]\n",
      "loss: 4.178587  [ 9920/50000]\n",
      "loss: 4.075540  [10240/50000]\n",
      "loss: 3.949300  [10560/50000]\n",
      "loss: 3.894503  [10880/50000]\n",
      "loss: 3.865153  [11200/50000]\n",
      "loss: 3.783031  [11520/50000]\n",
      "loss: 3.745574  [11840/50000]\n",
      "loss: 3.738131  [12160/50000]\n",
      "loss: 3.630290  [12480/50000]\n",
      "loss: 3.583547  [12800/50000]\n",
      "loss: 3.490114  [13120/50000]\n",
      "loss: 3.482683  [13440/50000]\n",
      "loss: 3.425151  [13760/50000]\n",
      "loss: 3.386449  [14080/50000]\n",
      "loss: 3.414742  [14400/50000]\n",
      "loss: 3.339644  [14720/50000]\n",
      "loss: 3.238237  [15040/50000]\n",
      "loss: 3.222388  [15360/50000]\n",
      "loss: 3.204482  [15680/50000]\n",
      "loss: 3.201282  [16000/50000]\n",
      "loss: 3.205864  [16320/50000]\n",
      "loss: 3.005692  [16640/50000]\n",
      "loss: 3.041068  [16960/50000]\n",
      "loss: 3.059496  [17280/50000]\n",
      "loss: 3.028563  [17600/50000]\n",
      "loss: 3.014560  [17920/50000]\n",
      "loss: 2.932761  [18240/50000]\n",
      "loss: 2.969270  [18560/50000]\n",
      "loss: 2.908358  [18880/50000]\n",
      "loss: 2.873616  [19200/50000]\n",
      "loss: 2.799400  [19520/50000]\n",
      "loss: 2.847441  [19840/50000]\n",
      "loss: 2.807019  [20160/50000]\n",
      "loss: 2.879511  [20480/50000]\n",
      "loss: 2.806741  [20800/50000]\n",
      "loss: 2.816928  [21120/50000]\n",
      "loss: 2.837506  [21440/50000]\n",
      "loss: 2.772204  [21760/50000]\n",
      "loss: 2.766054  [22080/50000]\n",
      "loss: 2.765310  [22400/50000]\n",
      "loss: 2.804952  [22720/50000]\n",
      "loss: 2.666745  [23040/50000]\n",
      "loss: 2.734762  [23360/50000]\n",
      "loss: 2.702515  [23680/50000]\n",
      "loss: 2.695409  [24000/50000]\n",
      "loss: 2.643842  [24320/50000]\n",
      "loss: 2.624305  [24640/50000]\n",
      "loss: 2.648416  [24960/50000]\n",
      "loss: 2.691309  [25280/50000]\n",
      "loss: 2.621525  [25600/50000]\n",
      "loss: 2.582154  [25920/50000]\n",
      "loss: 2.618838  [26240/50000]\n",
      "loss: 2.643928  [26560/50000]\n",
      "loss: 2.608725  [26880/50000]\n",
      "loss: 2.621089  [27200/50000]\n",
      "loss: 2.612933  [27520/50000]\n",
      "loss: 2.596266  [27840/50000]\n",
      "loss: 2.573486  [28160/50000]\n",
      "loss: 2.549019  [28480/50000]\n",
      "loss: 2.551696  [28800/50000]\n",
      "loss: 2.521208  [29120/50000]\n",
      "loss: 2.525082  [29440/50000]\n",
      "loss: 2.521996  [29760/50000]\n",
      "loss: 2.578901  [30080/50000]\n",
      "loss: 2.555890  [30400/50000]\n",
      "loss: 2.521444  [30720/50000]\n",
      "loss: 2.453611  [31040/50000]\n",
      "loss: 2.473039  [31360/50000]\n",
      "loss: 2.525564  [31680/50000]\n",
      "loss: 2.509769  [32000/50000]\n",
      "loss: 2.471511  [32320/50000]\n",
      "loss: 2.371081  [32640/50000]\n",
      "loss: 2.396285  [32960/50000]\n",
      "loss: 2.521440  [33280/50000]\n",
      "loss: 2.377456  [33600/50000]\n",
      "loss: 2.486677  [33920/50000]\n",
      "loss: 2.474432  [34240/50000]\n",
      "loss: 2.404565  [34560/50000]\n",
      "loss: 2.448506  [34880/50000]\n",
      "loss: 2.379506  [35200/50000]\n",
      "loss: 2.441820  [35520/50000]\n",
      "loss: 2.454441  [35840/50000]\n",
      "loss: 2.463402  [36160/50000]\n",
      "loss: 2.406024  [36480/50000]\n",
      "loss: 2.378237  [36800/50000]\n",
      "loss: 2.427451  [37120/50000]\n",
      "loss: 2.425776  [37440/50000]\n",
      "loss: 2.418063  [37760/50000]\n",
      "loss: 2.351895  [38080/50000]\n",
      "loss: 2.388587  [38400/50000]\n",
      "loss: 2.313766  [38720/50000]\n",
      "loss: 2.379954  [39040/50000]\n",
      "loss: 2.441426  [39360/50000]\n",
      "loss: 2.352187  [39680/50000]\n",
      "loss: 2.298936  [40000/50000]\n",
      "loss: 2.389098  [40320/50000]\n",
      "loss: 2.279571  [40640/50000]\n",
      "loss: 2.322176  [40960/50000]\n",
      "loss: 2.369070  [41280/50000]\n",
      "loss: 2.332463  [41600/50000]\n",
      "loss: 2.347096  [41920/50000]\n",
      "loss: 2.315927  [42240/50000]\n",
      "loss: 2.363367  [42560/50000]\n",
      "loss: 2.377079  [42880/50000]\n",
      "loss: 2.352903  [43200/50000]\n",
      "loss: 2.329323  [43520/50000]\n",
      "loss: 2.347136  [43840/50000]\n",
      "loss: 2.294172  [44160/50000]\n",
      "loss: 2.395942  [44480/50000]\n",
      "loss: 2.402333  [44800/50000]\n",
      "loss: 2.182384  [45120/50000]\n",
      "loss: 2.333590  [45440/50000]\n",
      "loss: 2.349017  [45760/50000]\n",
      "loss: 2.341094  [46080/50000]\n",
      "loss: 2.272391  [46400/50000]\n",
      "loss: 2.355566  [46720/50000]\n",
      "loss: 2.250610  [47040/50000]\n",
      "loss: 2.306793  [47360/50000]\n",
      "loss: 2.272103  [47680/50000]\n",
      "loss: 2.351492  [48000/50000]\n",
      "loss: 2.218689  [48320/50000]\n",
      "loss: 2.289934  [48640/50000]\n",
      "loss: 2.119523  [48960/50000]\n",
      "loss: 2.259819  [49280/50000]\n",
      "loss: 2.236055  [49600/50000]\n",
      "loss: 2.194420  [49920/50000]\n"
     ]
    }
   ],
   "source": [
    "import oneflow as flow\n",
    "import oneflow.nn as nn\n",
    "import flowvision\n",
    "import flowvision.transforms as transforms\n",
    "\n",
    "BATCH_SIZE=64\n",
    "EPOCH_NUM = 1\n",
    "\n",
    "DEVICE = \"cuda\" if flow.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(DEVICE))\n",
    "\n",
    "training_data = flowvision.datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True,\n",
    "    source_url=\"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/cifar/cifar-10-python.tar.gz\",\n",
    ")\n",
    "\n",
    "train_dataloader = flow.utils.data.DataLoader(\n",
    "    training_data, BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "model = flowvision.models.mobilenet_v2().to(DEVICE)\n",
    "model.classifer = nn.Sequential(nn.Dropout(0.2), nn.Linear(model.last_channel, 10))\n",
    "model.train()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = flow.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "for t in range(EPOCH_NUM):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    size = len(train_dataloader.dataset)\n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current = batch * BATCH_SIZE\n",
    "        if batch % 5 == 0:\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```text\n",
    "loss: 6.921304  [    0/50000]\n",
    "loss: 6.824391  [  320/50000]\n",
    "loss: 6.688272  [  640/50000]\n",
    "loss: 6.644351  [  960/50000]\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneFlow 的 Graph 模式\n",
    "\n",
    "### 自定义一个 Graph\n",
    "\n",
    "OneFlow 提供了 [nn.Graph](https://oneflow.readthedocs.io/en/master/graph.html) 基类。用户可以通过继承它，自定义 Graph 类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oneflow as flow\n",
    "import oneflow.nn as nn\n",
    "\n",
    "class ModuleMyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(flow.randn(in_features, out_features))\n",
    "        self.bias = nn.Parameter(flow.randn(out_features))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return flow.matmul(input, self.weight) + self.bias\n",
    "\n",
    "linear_model = ModuleMyLinear(4, 3)\n",
    "\n",
    "class GraphMyLinear(nn.Graph):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.model = linear_model\n",
    "\n",
    "  def build(self, input):\n",
    "    return self.model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上简单的例子，包含了自定义 Graph 所需的重要步骤：\n",
    "\n",
    "- 继承 `nn.Graph`\n",
    "- 在 `__init__` 最开始调用 `super().__init__()`，让 OneFlow 完成 Graph 必要的初始化工作\n",
    "- 在 `__init__` 中复用 Eager 模式下的 `nn.Module` 对象（`self.model = model`）\n",
    "- 在 `build` 中描述计算过程\n",
    "\n",
    "然后，就可以实例化并调用 Graph。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.2023,  2.9805, -2.6793]], dtype=oneflow.float32)\n"
     ]
    }
   ],
   "source": [
    "graph_mylinear = GraphMyLinear()\n",
    "input = flow.randn(1, 4)\n",
    "out = graph_mylinear(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```text\n",
    "tensor([[-0.3298, -3.7907,  0.1661]], dtype=oneflow.float32)\n",
    "```\n",
    "\n",
    "注意，Graph 与 Module 类似，对象本身是可调用的，并且 **不推荐** 显式调用 `build` 方法。Graph 可以直接复用已经定义好的 Module。因此，用户可以直接参考 [搭建神经网络](https://oneflow.cloud/drill/#/project/public/code?id=7e351fbd41fef653f26078e91915e76d) 中的内容搭建好神经网络，然后在 Graph 的 `__init__` 中将 Module 设置为 Graph 的成员即可。\n",
    "\n",
    "比如，直接使用以上 Eager 模式示例的 `model`，作为网络结构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelGraph(flow.nn.Graph):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = linear_model\n",
    "\n",
    "    def build(self, x, y):\n",
    "        y_pred = self.model(x)\n",
    "        return loss\n",
    "\n",
    "model_graph = ModelGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与 Module 的显著区别在于，Graph 使用 `build` 而不是 `forward` 方法描述计算过程，这是因为 `build` 不仅可以包含前向计算，还可以设置 `loss`，优化器等，在下文会看到使用 Graph 做训练的实际例子。\n",
    "\n",
    "### 使用 Graph 做预测\n",
    "\n",
    "以下 Graph 做预测的例子，直接使用了本文开始 Eager 模式训练好的 module。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphMobileNetV2(flow.nn.Graph):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def build(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "graph_mobile_net_v2 = GraphMobileNetV2()\n",
    "\n",
    "x, _ = next(iter(train_dataloader))\n",
    "x = x.to(DEVICE)\n",
    "y_pred = graph_mobile_net_v2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 Graph 做训练\n",
    "\n",
    "可以直接使用 Graph 做训练，以下为详细代码。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Files already downloaded and verified\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 6.886776  [    0/50000]\n",
      "loss: 6.795093  [  320/50000]\n",
      "loss: 6.681765  [  640/50000]\n",
      "loss: 6.608294  [  960/50000]\n",
      "loss: 6.472834  [ 1280/50000]\n",
      "loss: 6.422523  [ 1600/50000]\n",
      "loss: 6.288316  [ 1920/50000]\n",
      "loss: 6.232995  [ 2240/50000]\n",
      "loss: 6.150653  [ 2560/50000]\n",
      "loss: 5.990071  [ 2880/50000]\n",
      "loss: 5.941906  [ 3200/50000]\n",
      "loss: 5.825718  [ 3520/50000]\n",
      "loss: 5.784274  [ 3840/50000]\n",
      "loss: 5.615654  [ 4160/50000]\n",
      "loss: 5.537690  [ 4480/50000]\n",
      "loss: 5.512438  [ 4800/50000]\n",
      "loss: 5.317544  [ 5120/50000]\n",
      "loss: 5.221940  [ 5440/50000]\n",
      "loss: 5.177809  [ 5760/50000]\n",
      "loss: 5.080072  [ 6080/50000]\n",
      "loss: 4.994734  [ 6400/50000]\n",
      "loss: 4.855094  [ 6720/50000]\n",
      "loss: 4.887631  [ 7040/50000]\n",
      "loss: 4.708749  [ 7360/50000]\n",
      "loss: 4.688088  [ 7680/50000]\n",
      "loss: 4.639349  [ 8000/50000]\n",
      "loss: 4.536548  [ 8320/50000]\n",
      "loss: 4.420924  [ 8640/50000]\n",
      "loss: 4.349590  [ 8960/50000]\n",
      "loss: 4.264233  [ 9280/50000]\n",
      "loss: 4.192795  [ 9600/50000]\n",
      "loss: 4.144549  [ 9920/50000]\n",
      "loss: 4.117109  [10240/50000]\n",
      "loss: 4.013820  [10560/50000]\n",
      "loss: 4.052792  [10880/50000]\n",
      "loss: 3.820897  [11200/50000]\n",
      "loss: 3.754167  [11520/50000]\n",
      "loss: 3.725642  [11840/50000]\n",
      "loss: 3.698378  [12160/50000]\n",
      "loss: 3.640182  [12480/50000]\n",
      "loss: 3.538017  [12800/50000]\n",
      "loss: 3.496482  [13120/50000]\n",
      "loss: 3.433362  [13440/50000]\n",
      "loss: 3.489830  [13760/50000]\n",
      "loss: 3.419701  [14080/50000]\n",
      "loss: 3.363929  [14400/50000]\n",
      "loss: 3.306658  [14720/50000]\n",
      "loss: 3.302694  [15040/50000]\n",
      "loss: 3.261419  [15360/50000]\n",
      "loss: 3.227576  [15680/50000]\n",
      "loss: 3.142323  [16000/50000]\n",
      "loss: 3.146419  [16320/50000]\n",
      "loss: 3.234347  [16640/50000]\n",
      "loss: 3.057640  [16960/50000]\n",
      "loss: 3.070544  [17280/50000]\n",
      "loss: 3.064353  [17600/50000]\n",
      "loss: 2.988178  [17920/50000]\n",
      "loss: 3.008727  [18240/50000]\n",
      "loss: 2.874135  [18560/50000]\n",
      "loss: 2.891906  [18880/50000]\n",
      "loss: 2.901055  [19200/50000]\n",
      "loss: 2.858563  [19520/50000]\n",
      "loss: 2.854941  [19840/50000]\n",
      "loss: 2.835297  [20160/50000]\n",
      "loss: 2.804049  [20480/50000]\n",
      "loss: 2.834850  [20800/50000]\n",
      "loss: 2.778445  [21120/50000]\n",
      "loss: 2.816111  [21440/50000]\n",
      "loss: 2.736905  [21760/50000]\n",
      "loss: 2.803172  [22080/50000]\n",
      "loss: 2.756530  [22400/50000]\n",
      "loss: 2.768412  [22720/50000]\n",
      "loss: 2.719205  [23040/50000]\n",
      "loss: 2.682514  [23360/50000]\n",
      "loss: 2.681726  [23680/50000]\n",
      "loss: 2.633095  [24000/50000]\n",
      "loss: 2.729096  [24320/50000]\n",
      "loss: 2.709026  [24640/50000]\n",
      "loss: 2.595726  [24960/50000]\n",
      "loss: 2.573332  [25280/50000]\n",
      "loss: 2.585768  [25600/50000]\n",
      "loss: 2.569645  [25920/50000]\n",
      "loss: 2.536571  [26240/50000]\n",
      "loss: 2.584811  [26560/50000]\n",
      "loss: 2.559647  [26880/50000]\n",
      "loss: 2.540365  [27200/50000]\n",
      "loss: 2.584946  [27520/50000]\n",
      "loss: 2.441565  [27840/50000]\n",
      "loss: 2.521151  [28160/50000]\n",
      "loss: 2.570940  [28480/50000]\n",
      "loss: 2.539874  [28800/50000]\n",
      "loss: 2.507461  [29120/50000]\n",
      "loss: 2.516834  [29440/50000]\n",
      "loss: 2.550288  [29760/50000]\n",
      "loss: 2.495902  [30080/50000]\n",
      "loss: 2.410911  [30400/50000]\n",
      "loss: 2.624653  [30720/50000]\n",
      "loss: 2.489035  [31040/50000]\n",
      "loss: 2.402662  [31360/50000]\n",
      "loss: 2.406259  [31680/50000]\n",
      "loss: 2.496597  [32000/50000]\n",
      "loss: 2.458573  [32320/50000]\n",
      "loss: 2.410210  [32640/50000]\n",
      "loss: 2.389126  [32960/50000]\n",
      "loss: 2.580502  [33280/50000]\n",
      "loss: 2.423331  [33600/50000]\n",
      "loss: 2.498325  [33920/50000]\n",
      "loss: 2.405819  [34240/50000]\n",
      "loss: 2.336037  [34560/50000]\n",
      "loss: 2.480188  [34880/50000]\n",
      "loss: 2.394475  [35200/50000]\n",
      "loss: 2.305721  [35520/50000]\n",
      "loss: 2.392916  [35840/50000]\n",
      "loss: 2.428489  [36160/50000]\n",
      "loss: 2.322629  [36480/50000]\n",
      "loss: 2.308502  [36800/50000]\n",
      "loss: 2.308002  [37120/50000]\n",
      "loss: 2.368910  [37440/50000]\n",
      "loss: 2.343117  [37760/50000]\n",
      "loss: 2.405287  [38080/50000]\n",
      "loss: 2.391244  [38400/50000]\n",
      "loss: 2.259197  [38720/50000]\n",
      "loss: 2.392688  [39040/50000]\n",
      "loss: 2.272597  [39360/50000]\n",
      "loss: 2.306551  [39680/50000]\n",
      "loss: 2.324215  [40000/50000]\n",
      "loss: 2.349412  [40320/50000]\n",
      "loss: 2.408329  [40640/50000]\n",
      "loss: 2.322548  [40960/50000]\n",
      "loss: 2.288723  [41280/50000]\n",
      "loss: 2.272868  [41600/50000]\n",
      "loss: 2.369234  [41920/50000]\n",
      "loss: 2.269620  [42240/50000]\n",
      "loss: 2.338036  [42560/50000]\n",
      "loss: 2.354719  [42880/50000]\n",
      "loss: 2.340951  [43200/50000]\n",
      "loss: 2.417522  [43520/50000]\n",
      "loss: 2.215576  [43840/50000]\n",
      "loss: 2.281292  [44160/50000]\n",
      "loss: 2.377904  [44480/50000]\n",
      "loss: 2.232479  [44800/50000]\n",
      "loss: 2.236799  [45120/50000]\n",
      "loss: 2.296223  [45440/50000]\n",
      "loss: 2.310424  [45760/50000]\n",
      "loss: 2.243718  [46080/50000]\n",
      "loss: 2.216802  [46400/50000]\n",
      "loss: 2.321995  [46720/50000]\n",
      "loss: 2.244663  [47040/50000]\n",
      "loss: 2.354085  [47360/50000]\n",
      "loss: 2.222795  [47680/50000]\n",
      "loss: 2.326585  [48000/50000]\n",
      "loss: 2.276786  [48320/50000]\n",
      "loss: 2.315252  [48640/50000]\n",
      "loss: 2.285625  [48960/50000]\n",
      "loss: 2.330630  [49280/50000]\n",
      "loss: 2.240008  [49600/50000]\n",
      "loss: 2.339387  [49920/50000]\n"
     ]
    }
   ],
   "source": [
    "import oneflow as flow\n",
    "import oneflow.nn as nn\n",
    "import flowvision\n",
    "import flowvision.transforms as transforms\n",
    "\n",
    "BATCH_SIZE=64\n",
    "EPOCH_NUM = 1\n",
    "\n",
    "DEVICE = \"cuda\" if flow.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(DEVICE))\n",
    "\n",
    "training_data = flowvision.datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "train_dataloader = flow.utils.data.DataLoader(\n",
    "    training_data, BATCH_SIZE, shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "model = flowvision.models.mobilenet_v2().to(DEVICE)\n",
    "model.classifer = nn.Sequential(nn.Dropout(0.2), nn.Linear(model.last_channel, 10))\n",
    "model.train()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = flow.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "class GraphMobileNetV2(flow.nn.Graph):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.add_optimizer(optimizer)\n",
    "\n",
    "    def build(self, x, y):\n",
    "        y_pred = self.model(x)\n",
    "        loss = self.loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "\n",
    "graph_mobile_net_v2 = GraphMobileNetV2()\n",
    "# graph_mobile_net_v2.debug()\n",
    "\n",
    "for t in range(EPOCH_NUM):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    size = len(train_dataloader.dataset)\n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        loss = graph_mobile_net_v2(x, y)\n",
    "        current = batch * BATCH_SIZE\n",
    "        if batch % 5 == 0:\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与 Graph 做预测的代码做比较，可以发现，只有以下几点是 Graph 做训练时特有的：\n",
    "\n",
    "```python\n",
    "# Optimizer\n",
    "optimizer = flow.optim.SGD(model.parameters(), lr=1e-3) # (1)\n",
    "\n",
    "# The MobileNetV2 Graph\n",
    "class GraphMobileNetV2(flow.nn.Graph):\n",
    "    def __init__(self):\n",
    "        # ...\n",
    "        self.add_optimizer(optimizer) # (2)\n",
    "\n",
    "    def build(self, x, y):\n",
    "        # ...\n",
    "        loss.backward() # (3)\n",
    "        # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 构造 optimizer 对象，这点和 [反向传播与 optimizer](https://oneflow.cloud/drill/#/project/public/code?id=d05f6d1ec8e587456389b95270d3b7ac) 介绍的 Eager 模式的使用方法是完全一致的。\n",
    "2. 在 Graph 类的 `__init__` 中，调用 `self.add_optimizer` 方法，将上一步构造的 optimizer 对象添加进 Graph 中。\n",
    "3. 在 Graph 类的 `build` 中调用 `backward`，触发反向传播\n",
    "\n",
    "### Graph 调试\n",
    "\n",
    "当前输出 Graph 的调试信息共有两种方式。**第一种** 可以调用 `print` 打印 Graph 对象，输出 Graph 对象的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GRAPH:GraphMobileNetV2_2:GraphMobileNetV2): (\n",
      "  (CONFIG:config:GraphConfig(training=True, ))\n",
      "  (INPUT:_GraphMobileNetV2_2_input.0.0_2:tensor(..., device='cuda:0', size=(64, 3, 32, 32), dtype=oneflow.float32))\n",
      "  (INPUT:_GraphMobileNetV2_2_input.0.1_3:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.int64))\n",
      "  (MODULE:model:MobileNetV2()): (\n",
      "    (INPUT:_model_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 3, 32, 32),\n",
      "           dtype=oneflow.float32))\n",
      "    (MODULE:model.features:Sequential()): (\n",
      "      (INPUT:_model.features_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 3, 32, 32),\n",
      "             dtype=oneflow.float32))\n",
      "      (MODULE:model.features.0:ConvBNActivation()): (\n",
      "        (INPUT:_model.features.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 3, 32, 32),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.0.0:Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)): (\n",
      "          (INPUT:_model.features.0.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 3, 32, 32),\n",
      "                 dtype=oneflow.float32))\n",
      "          (PARAMETER:model.features.0.0.weight:tensor(..., device='cuda:0', size=(32, 3, 3, 3), dtype=oneflow.float32,\n",
      "                 requires_grad=True)): ()\n",
      "          (OUTPUT:_model.features.0.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (MODULE:model.features.0.1:BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "          (INPUT:_model.features.0.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16),\n",
      "                 dtype=oneflow.float32))\n",
      "          (PARAMETER:model.features.0.1.weight:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32,\n",
      "                 requires_grad=True)): ()\n",
      "          (PARAMETER:model.features.0.1.bias:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32,\n",
      "                 requires_grad=True)): ()\n",
      "          (BUFFER:model.features.0.1.running_mean:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32)): ()\n",
      "          (BUFFER:model.features.0.1.running_var:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32)): ()\n",
      "          (OUTPUT:_model.features.0.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (MODULE:model.features.0.2:ReLU6(inplace=True)): (\n",
      "          (INPUT:_model.features.0.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16),\n",
      "                 dtype=oneflow.float32))\n",
      "          (OUTPUT:_model.features.0.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.features.1:InvertedResidual()): (\n",
      "        (INPUT:_model.features.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.1.conv:Sequential()): (\n",
      "          (INPUT:_model.features.1.conv_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16),\n",
      "                 dtype=oneflow.float32))\n",
      "          (MODULE:model.features.1.conv.0:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.1.conv.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.1.conv.0.0:Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)): (\n",
      "              (INPUT:_model.features.1.conv.0.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.1.conv.0.0.weight:tensor(..., device='cuda:0', size=(32, 1, 3, 3), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.1.conv.0.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.1.conv.0.1:BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.1.conv.0.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.1.conv.0.1.weight:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.1.conv.0.1.bias:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.1.conv.0.1.running_mean:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.1.conv.0.1.running_var:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.1.conv.0.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.1.conv.0.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.1.conv.0.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.1.conv.0.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.1.conv.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.1.conv.1:Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "            (INPUT:_model.features.1.conv.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.1.conv.1.weight:tensor(..., device='cuda:0', size=(16, 32, 1, 1), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (OUTPUT:_model.features.1.conv.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 16, 16, 16),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.1.conv.2:BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "            (INPUT:_model.features.1.conv.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 16, 16, 16),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.1.conv.2.weight:tensor(..., device='cuda:0', size=(16,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (PARAMETER:model.features.1.conv.2.bias:tensor(..., device='cuda:0', size=(16,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (BUFFER:model.features.1.conv.2.running_mean:tensor(..., device='cuda:0', size=(16,), dtype=oneflow.float32)): ()\n",
      "            (BUFFER:model.features.1.conv.2.running_var:tensor(..., device='cuda:0', size=(16,), dtype=oneflow.float32)): ()\n",
      "            (OUTPUT:_model.features.1.conv.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 16, 16, 16),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (OUTPUT:_model.features.1.conv_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 16, 16, 16),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 16, 16, 16),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.features.2:InvertedResidual()): (\n",
      "        (INPUT:_model.features.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 16, 16, 16),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.2.conv:Sequential()): (\n",
      "          (INPUT:_model.features.2.conv_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 16, 16, 16),\n",
      "                 dtype=oneflow.float32))\n",
      "          (MODULE:model.features.2.conv.0:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.2.conv.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 16, 16, 16),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.2.conv.0.0:Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "              (INPUT:_model.features.2.conv.0.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 16, 16, 16),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.2.conv.0.0.weight:tensor(..., device='cuda:0', size=(96, 16, 1, 1), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.2.conv.0.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 16, 16),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.2.conv.0.1:BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.2.conv.0.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 16, 16),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.2.conv.0.1.weight:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.2.conv.0.1.bias:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.2.conv.0.1.running_mean:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.2.conv.0.1.running_var:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.2.conv.0.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 16, 16),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.2.conv.0.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.2.conv.0.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 16, 16),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.2.conv.0.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 16, 16),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.2.conv.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 16, 16),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.2.conv.1:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.2.conv.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 16, 16),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.2.conv.1.0:Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)): (\n",
      "              (INPUT:_model.features.2.conv.1.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 16, 16),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.2.conv.1.0.weight:tensor(..., device='cuda:0', size=(96, 1, 3, 3), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.2.conv.1.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.2.conv.1.1:BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.2.conv.1.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.2.conv.1.1.weight:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.2.conv.1.1.bias:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.2.conv.1.1.running_mean:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.2.conv.1.1.running_var:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.2.conv.1.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.2.conv.1.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.2.conv.1.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.2.conv.1.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.2.conv.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 8, 8),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.2.conv.2:Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "            (INPUT:_model.features.2.conv.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 8, 8),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.2.conv.2.weight:tensor(..., device='cuda:0', size=(24, 96, 1, 1), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (OUTPUT:_model.features.2.conv.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 24, 8, 8),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.2.conv.3:BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "            (INPUT:_model.features.2.conv.3_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 24, 8, 8),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.2.conv.3.weight:tensor(..., device='cuda:0', size=(24,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (PARAMETER:model.features.2.conv.3.bias:tensor(..., device='cuda:0', size=(24,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (BUFFER:model.features.2.conv.3.running_mean:tensor(..., device='cuda:0', size=(24,), dtype=oneflow.float32)): ()\n",
      "            (BUFFER:model.features.2.conv.3.running_var:tensor(..., device='cuda:0', size=(24,), dtype=oneflow.float32)): ()\n",
      "            (OUTPUT:_model.features.2.conv.3_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 24, 8, 8),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (OUTPUT:_model.features.2.conv_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 24, 8, 8),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 24, 8, 8),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.features.3:InvertedResidual()): (\n",
      "        (INPUT:_model.features.3_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 24, 8, 8),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.3.conv:Sequential()): (\n",
      "          (INPUT:_model.features.3.conv_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 24, 8, 8),\n",
      "                 dtype=oneflow.float32))\n",
      "          (MODULE:model.features.3.conv.0:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.3.conv.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 24, 8, 8),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.3.conv.0.0:Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "              (INPUT:_model.features.3.conv.0.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 24, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.3.conv.0.0.weight:tensor(..., device='cuda:0', size=(144, 24, 1, 1), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.3.conv.0.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.3.conv.0.1:BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.3.conv.0.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.3.conv.0.1.weight:tensor(..., device='cuda:0', size=(144,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.3.conv.0.1.bias:tensor(..., device='cuda:0', size=(144,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.3.conv.0.1.running_mean:tensor(..., device='cuda:0', size=(144,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.3.conv.0.1.running_var:tensor(..., device='cuda:0', size=(144,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.3.conv.0.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.3.conv.0.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.3.conv.0.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.3.conv.0.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.3.conv.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.3.conv.1:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.3.conv.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.3.conv.1.0:Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)): (\n",
      "              (INPUT:_model.features.3.conv.1.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.3.conv.1.0.weight:tensor(..., device='cuda:0', size=(144, 1, 3, 3), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.3.conv.1.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.3.conv.1.1:BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.3.conv.1.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.3.conv.1.1.weight:tensor(..., device='cuda:0', size=(144,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.3.conv.1.1.bias:tensor(..., device='cuda:0', size=(144,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.3.conv.1.1.running_mean:tensor(..., device='cuda:0', size=(144,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.3.conv.1.1.running_var:tensor(..., device='cuda:0', size=(144,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.3.conv.1.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.3.conv.1.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.3.conv.1.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.3.conv.1.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.3.conv.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.3.conv.2:Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "            (INPUT:_model.features.3.conv.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.3.conv.2.weight:tensor(..., device='cuda:0', size=(24, 144, 1, 1), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (OUTPUT:_model.features.3.conv.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 24, 8, 8),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.3.conv.3:BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "            (INPUT:_model.features.3.conv.3_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 24, 8, 8),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.3.conv.3.weight:tensor(..., device='cuda:0', size=(24,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (PARAMETER:model.features.3.conv.3.bias:tensor(..., device='cuda:0', size=(24,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (BUFFER:model.features.3.conv.3.running_mean:tensor(..., device='cuda:0', size=(24,), dtype=oneflow.float32)): ()\n",
      "            (BUFFER:model.features.3.conv.3.running_var:tensor(..., device='cuda:0', size=(24,), dtype=oneflow.float32)): ()\n",
      "            (OUTPUT:_model.features.3.conv.3_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 24, 8, 8),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (OUTPUT:_model.features.3.conv_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 24, 8, 8),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.3_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 24, 8, 8),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.features.4:InvertedResidual()): (\n",
      "        (INPUT:_model.features.4_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 24, 8, 8),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.4.conv:Sequential()): (\n",
      "          (INPUT:_model.features.4.conv_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 24, 8, 8),\n",
      "                 dtype=oneflow.float32))\n",
      "          (MODULE:model.features.4.conv.0:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.4.conv.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 24, 8, 8),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.4.conv.0.0:Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "              (INPUT:_model.features.4.conv.0.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 24, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.4.conv.0.0.weight:tensor(..., device='cuda:0', size=(144, 24, 1, 1), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.4.conv.0.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.4.conv.0.1:BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.4.conv.0.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.4.conv.0.1.weight:tensor(..., device='cuda:0', size=(144,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.4.conv.0.1.bias:tensor(..., device='cuda:0', size=(144,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.4.conv.0.1.running_mean:tensor(..., device='cuda:0', size=(144,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.4.conv.0.1.running_var:tensor(..., device='cuda:0', size=(144,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.4.conv.0.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.4.conv.0.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.4.conv.0.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.4.conv.0.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.4.conv.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.4.conv.1:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.4.conv.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.4.conv.1.0:Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)): (\n",
      "              (INPUT:_model.features.4.conv.1.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 8, 8),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.4.conv.1.0.weight:tensor(..., device='cuda:0', size=(144, 1, 3, 3), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.4.conv.1.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.4.conv.1.1:BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.4.conv.1.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.4.conv.1.1.weight:tensor(..., device='cuda:0', size=(144,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.4.conv.1.1.bias:tensor(..., device='cuda:0', size=(144,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.4.conv.1.1.running_mean:tensor(..., device='cuda:0', size=(144,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.4.conv.1.1.running_var:tensor(..., device='cuda:0', size=(144,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.4.conv.1.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.4.conv.1.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.4.conv.1.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.4.conv.1.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.4.conv.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.4.conv.2:Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "            (INPUT:_model.features.4.conv.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 144, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.4.conv.2.weight:tensor(..., device='cuda:0', size=(32, 144, 1, 1), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (OUTPUT:_model.features.4.conv.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.4.conv.3:BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "            (INPUT:_model.features.4.conv.3_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.4.conv.3.weight:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (PARAMETER:model.features.4.conv.3.bias:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (BUFFER:model.features.4.conv.3.running_mean:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32)): ()\n",
      "            (BUFFER:model.features.4.conv.3.running_var:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32)): ()\n",
      "            (OUTPUT:_model.features.4.conv.3_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (OUTPUT:_model.features.4.conv_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.4_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.features.5:InvertedResidual()): (\n",
      "        (INPUT:_model.features.5_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.5.conv:Sequential()): (\n",
      "          (INPUT:_model.features.5.conv_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                 dtype=oneflow.float32))\n",
      "          (MODULE:model.features.5.conv.0:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.5.conv.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.5.conv.0.0:Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "              (INPUT:_model.features.5.conv.0.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.5.conv.0.0.weight:tensor(..., device='cuda:0', size=(192, 32, 1, 1), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.5.conv.0.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.5.conv.0.1:BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.5.conv.0.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.5.conv.0.1.weight:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.5.conv.0.1.bias:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.5.conv.0.1.running_mean:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.5.conv.0.1.running_var:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.5.conv.0.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.5.conv.0.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.5.conv.0.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.5.conv.0.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.5.conv.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.5.conv.1:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.5.conv.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.5.conv.1.0:Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)): (\n",
      "              (INPUT:_model.features.5.conv.1.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.5.conv.1.0.weight:tensor(..., device='cuda:0', size=(192, 1, 3, 3), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.5.conv.1.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.5.conv.1.1:BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.5.conv.1.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.5.conv.1.1.weight:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.5.conv.1.1.bias:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.5.conv.1.1.running_mean:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.5.conv.1.1.running_var:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.5.conv.1.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.5.conv.1.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.5.conv.1.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.5.conv.1.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.5.conv.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.5.conv.2:Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "            (INPUT:_model.features.5.conv.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.5.conv.2.weight:tensor(..., device='cuda:0', size=(32, 192, 1, 1), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (OUTPUT:_model.features.5.conv.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.5.conv.3:BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "            (INPUT:_model.features.5.conv.3_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.5.conv.3.weight:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (PARAMETER:model.features.5.conv.3.bias:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (BUFFER:model.features.5.conv.3.running_mean:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32)): ()\n",
      "            (BUFFER:model.features.5.conv.3.running_var:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32)): ()\n",
      "            (OUTPUT:_model.features.5.conv.3_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (OUTPUT:_model.features.5.conv_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.5_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.features.6:InvertedResidual()): (\n",
      "        (INPUT:_model.features.6_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.6.conv:Sequential()): (\n",
      "          (INPUT:_model.features.6.conv_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                 dtype=oneflow.float32))\n",
      "          (MODULE:model.features.6.conv.0:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.6.conv.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.6.conv.0.0:Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "              (INPUT:_model.features.6.conv.0.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.6.conv.0.0.weight:tensor(..., device='cuda:0', size=(192, 32, 1, 1), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.6.conv.0.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.6.conv.0.1:BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.6.conv.0.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.6.conv.0.1.weight:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.6.conv.0.1.bias:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.6.conv.0.1.running_mean:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.6.conv.0.1.running_var:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.6.conv.0.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.6.conv.0.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.6.conv.0.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.6.conv.0.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.6.conv.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.6.conv.1:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.6.conv.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.6.conv.1.0:Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)): (\n",
      "              (INPUT:_model.features.6.conv.1.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.6.conv.1.0.weight:tensor(..., device='cuda:0', size=(192, 1, 3, 3), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.6.conv.1.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.6.conv.1.1:BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.6.conv.1.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.6.conv.1.1.weight:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.6.conv.1.1.bias:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.6.conv.1.1.running_mean:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.6.conv.1.1.running_var:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.6.conv.1.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.6.conv.1.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.6.conv.1.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.6.conv.1.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.6.conv.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.6.conv.2:Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "            (INPUT:_model.features.6.conv.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.6.conv.2.weight:tensor(..., device='cuda:0', size=(32, 192, 1, 1), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (OUTPUT:_model.features.6.conv.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.6.conv.3:BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "            (INPUT:_model.features.6.conv.3_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.6.conv.3.weight:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (PARAMETER:model.features.6.conv.3.bias:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (BUFFER:model.features.6.conv.3.running_mean:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32)): ()\n",
      "            (BUFFER:model.features.6.conv.3.running_var:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32)): ()\n",
      "            (OUTPUT:_model.features.6.conv.3_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (OUTPUT:_model.features.6.conv_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.6_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.features.7:InvertedResidual()): (\n",
      "        (INPUT:_model.features.7_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.7.conv:Sequential()): (\n",
      "          (INPUT:_model.features.7.conv_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                 dtype=oneflow.float32))\n",
      "          (MODULE:model.features.7.conv.0:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.7.conv.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.7.conv.0.0:Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "              (INPUT:_model.features.7.conv.0.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.7.conv.0.0.weight:tensor(..., device='cuda:0', size=(192, 32, 1, 1), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.7.conv.0.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.7.conv.0.1:BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.7.conv.0.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.7.conv.0.1.weight:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.7.conv.0.1.bias:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.7.conv.0.1.running_mean:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.7.conv.0.1.running_var:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.7.conv.0.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.7.conv.0.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.7.conv.0.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.7.conv.0.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.7.conv.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.7.conv.1:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.7.conv.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.7.conv.1.0:Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)): (\n",
      "              (INPUT:_model.features.7.conv.1.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 4, 4),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.7.conv.1.0.weight:tensor(..., device='cuda:0', size=(192, 1, 3, 3), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.7.conv.1.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.7.conv.1.1:BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.7.conv.1.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.7.conv.1.1.weight:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.7.conv.1.1.bias:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.7.conv.1.1.running_mean:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.7.conv.1.1.running_var:tensor(..., device='cuda:0', size=(192,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.7.conv.1.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.7.conv.1.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.7.conv.1.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.7.conv.1.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.7.conv.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.7.conv.2:Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "            (INPUT:_model.features.7.conv.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 192, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.7.conv.2.weight:tensor(..., device='cuda:0', size=(64, 192, 1, 1), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (OUTPUT:_model.features.7.conv.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.7.conv.3:BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "            (INPUT:_model.features.7.conv.3_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.7.conv.3.weight:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (PARAMETER:model.features.7.conv.3.bias:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (BUFFER:model.features.7.conv.3.running_mean:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.float32)): ()\n",
      "            (BUFFER:model.features.7.conv.3.running_var:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.float32)): ()\n",
      "            (OUTPUT:_model.features.7.conv.3_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (OUTPUT:_model.features.7.conv_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.7_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.features.8:InvertedResidual()): (\n",
      "        (INPUT:_model.features.8_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.8.conv:Sequential()): (\n",
      "          (INPUT:_model.features.8.conv_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                 dtype=oneflow.float32))\n",
      "          (MODULE:model.features.8.conv.0:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.8.conv.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.8.conv.0.0:Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "              (INPUT:_model.features.8.conv.0.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.8.conv.0.0.weight:tensor(..., device='cuda:0', size=(384, 64, 1, 1), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.8.conv.0.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.8.conv.0.1:BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.8.conv.0.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.8.conv.0.1.weight:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.8.conv.0.1.bias:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.8.conv.0.1.running_mean:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.8.conv.0.1.running_var:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.8.conv.0.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.8.conv.0.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.8.conv.0.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.8.conv.0.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.8.conv.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.8.conv.1:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.8.conv.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.8.conv.1.0:Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)): (\n",
      "              (INPUT:_model.features.8.conv.1.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.8.conv.1.0.weight:tensor(..., device='cuda:0', size=(384, 1, 3, 3), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.8.conv.1.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.8.conv.1.1:BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.8.conv.1.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.8.conv.1.1.weight:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.8.conv.1.1.bias:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.8.conv.1.1.running_mean:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.8.conv.1.1.running_var:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.8.conv.1.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.8.conv.1.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.8.conv.1.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.8.conv.1.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.8.conv.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.8.conv.2:Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "            (INPUT:_model.features.8.conv.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.8.conv.2.weight:tensor(..., device='cuda:0', size=(64, 384, 1, 1), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (OUTPUT:_model.features.8.conv.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.8.conv.3:BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "            (INPUT:_model.features.8.conv.3_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.8.conv.3.weight:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (PARAMETER:model.features.8.conv.3.bias:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (BUFFER:model.features.8.conv.3.running_mean:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.float32)): ()\n",
      "            (BUFFER:model.features.8.conv.3.running_var:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.float32)): ()\n",
      "            (OUTPUT:_model.features.8.conv.3_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (OUTPUT:_model.features.8.conv_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.8_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.features.9:InvertedResidual()): (\n",
      "        (INPUT:_model.features.9_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.9.conv:Sequential()): (\n",
      "          (INPUT:_model.features.9.conv_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                 dtype=oneflow.float32))\n",
      "          (MODULE:model.features.9.conv.0:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.9.conv.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.9.conv.0.0:Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "              (INPUT:_model.features.9.conv.0.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.9.conv.0.0.weight:tensor(..., device='cuda:0', size=(384, 64, 1, 1), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.9.conv.0.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.9.conv.0.1:BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.9.conv.0.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.9.conv.0.1.weight:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.9.conv.0.1.bias:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.9.conv.0.1.running_mean:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.9.conv.0.1.running_var:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.9.conv.0.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.9.conv.0.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.9.conv.0.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.9.conv.0.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.9.conv.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.9.conv.1:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.9.conv.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.9.conv.1.0:Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)): (\n",
      "              (INPUT:_model.features.9.conv.1.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.9.conv.1.0.weight:tensor(..., device='cuda:0', size=(384, 1, 3, 3), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.9.conv.1.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.9.conv.1.1:BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.9.conv.1.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.9.conv.1.1.weight:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.9.conv.1.1.bias:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.9.conv.1.1.running_mean:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.9.conv.1.1.running_var:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.9.conv.1.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.9.conv.1.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.9.conv.1.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.9.conv.1.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.9.conv.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.9.conv.2:Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "            (INPUT:_model.features.9.conv.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.9.conv.2.weight:tensor(..., device='cuda:0', size=(64, 384, 1, 1), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (OUTPUT:_model.features.9.conv.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.9.conv.3:BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "            (INPUT:_model.features.9.conv.3_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.9.conv.3.weight:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (PARAMETER:model.features.9.conv.3.bias:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (BUFFER:model.features.9.conv.3.running_mean:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.float32)): ()\n",
      "            (BUFFER:model.features.9.conv.3.running_var:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.float32)): ()\n",
      "            (OUTPUT:_model.features.9.conv.3_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (OUTPUT:_model.features.9.conv_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.9_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.features.10:InvertedResidual()): (\n",
      "        (INPUT:_model.features.10_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.10.conv:Sequential()): (\n",
      "          (INPUT:_model.features.10.conv_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                 dtype=oneflow.float32))\n",
      "          (MODULE:model.features.10.conv.0:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.10.conv.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.10.conv.0.0:Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "              (INPUT:_model.features.10.conv.0.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.10.conv.0.0.weight:tensor(..., device='cuda:0', size=(384, 64, 1, 1), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.10.conv.0.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.10.conv.0.1:BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.10.conv.0.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.10.conv.0.1.weight:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.10.conv.0.1.bias:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.10.conv.0.1.running_mean:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.10.conv.0.1.running_var:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.10.conv.0.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.10.conv.0.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.10.conv.0.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.10.conv.0.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.10.conv.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.10.conv.1:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.10.conv.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.10.conv.1.0:Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)): (\n",
      "              (INPUT:_model.features.10.conv.1.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.10.conv.1.0.weight:tensor(..., device='cuda:0', size=(384, 1, 3, 3), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.10.conv.1.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.10.conv.1.1:BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.10.conv.1.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.10.conv.1.1.weight:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.10.conv.1.1.bias:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.10.conv.1.1.running_mean:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.10.conv.1.1.running_var:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.10.conv.1.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.10.conv.1.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.10.conv.1.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.10.conv.1.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.10.conv.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.10.conv.2:Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "            (INPUT:_model.features.10.conv.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.10.conv.2.weight:tensor(..., device='cuda:0', size=(64, 384, 1, 1), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (OUTPUT:_model.features.10.conv.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.10.conv.3:BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "            (INPUT:_model.features.10.conv.3_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.10.conv.3.weight:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (PARAMETER:model.features.10.conv.3.bias:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (BUFFER:model.features.10.conv.3.running_mean:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.float32)): ()\n",
      "            (BUFFER:model.features.10.conv.3.running_var:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.float32)): ()\n",
      "            (OUTPUT:_model.features.10.conv.3_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (OUTPUT:_model.features.10.conv_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.10_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.features.11:InvertedResidual()): (\n",
      "        (INPUT:_model.features.11_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.11.conv:Sequential()): (\n",
      "          (INPUT:_model.features.11.conv_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                 dtype=oneflow.float32))\n",
      "          (MODULE:model.features.11.conv.0:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.11.conv.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.11.conv.0.0:Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "              (INPUT:_model.features.11.conv.0.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 64, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.11.conv.0.0.weight:tensor(..., device='cuda:0', size=(384, 64, 1, 1), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.11.conv.0.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.11.conv.0.1:BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.11.conv.0.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.11.conv.0.1.weight:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.11.conv.0.1.bias:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.11.conv.0.1.running_mean:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.11.conv.0.1.running_var:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.11.conv.0.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.11.conv.0.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.11.conv.0.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.11.conv.0.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.11.conv.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.11.conv.1:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.11.conv.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.11.conv.1.0:Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)): (\n",
      "              (INPUT:_model.features.11.conv.1.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.11.conv.1.0.weight:tensor(..., device='cuda:0', size=(384, 1, 3, 3), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.11.conv.1.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.11.conv.1.1:BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.11.conv.1.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.11.conv.1.1.weight:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.11.conv.1.1.bias:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.11.conv.1.1.running_mean:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.11.conv.1.1.running_var:tensor(..., device='cuda:0', size=(384,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.11.conv.1.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.11.conv.1.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.11.conv.1.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.11.conv.1.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.11.conv.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.11.conv.2:Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "            (INPUT:_model.features.11.conv.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 384, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.11.conv.2.weight:tensor(..., device='cuda:0', size=(96, 384, 1, 1), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (OUTPUT:_model.features.11.conv.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.11.conv.3:BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "            (INPUT:_model.features.11.conv.3_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.11.conv.3.weight:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (PARAMETER:model.features.11.conv.3.bias:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (BUFFER:model.features.11.conv.3.running_mean:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32)): ()\n",
      "            (BUFFER:model.features.11.conv.3.running_var:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32)): ()\n",
      "            (OUTPUT:_model.features.11.conv.3_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (OUTPUT:_model.features.11.conv_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.11_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.features.12:InvertedResidual()): (\n",
      "        (INPUT:_model.features.12_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.12.conv:Sequential()): (\n",
      "          (INPUT:_model.features.12.conv_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                 dtype=oneflow.float32))\n",
      "          (MODULE:model.features.12.conv.0:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.12.conv.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.12.conv.0.0:Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "              (INPUT:_model.features.12.conv.0.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.12.conv.0.0.weight:tensor(..., device='cuda:0', size=(576, 96, 1, 1), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.12.conv.0.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.12.conv.0.1:BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.12.conv.0.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.12.conv.0.1.weight:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.12.conv.0.1.bias:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.12.conv.0.1.running_mean:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.12.conv.0.1.running_var:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.12.conv.0.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.12.conv.0.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.12.conv.0.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.12.conv.0.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.12.conv.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.12.conv.1:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.12.conv.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.12.conv.1.0:Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)): (\n",
      "              (INPUT:_model.features.12.conv.1.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.12.conv.1.0.weight:tensor(..., device='cuda:0', size=(576, 1, 3, 3), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.12.conv.1.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.12.conv.1.1:BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.12.conv.1.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.12.conv.1.1.weight:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.12.conv.1.1.bias:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.12.conv.1.1.running_mean:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.12.conv.1.1.running_var:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.12.conv.1.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.12.conv.1.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.12.conv.1.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.12.conv.1.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.12.conv.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.12.conv.2:Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "            (INPUT:_model.features.12.conv.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.12.conv.2.weight:tensor(..., device='cuda:0', size=(96, 576, 1, 1), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (OUTPUT:_model.features.12.conv.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.12.conv.3:BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "            (INPUT:_model.features.12.conv.3_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.12.conv.3.weight:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (PARAMETER:model.features.12.conv.3.bias:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (BUFFER:model.features.12.conv.3.running_mean:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32)): ()\n",
      "            (BUFFER:model.features.12.conv.3.running_var:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32)): ()\n",
      "            (OUTPUT:_model.features.12.conv.3_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (OUTPUT:_model.features.12.conv_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.12_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.features.13:InvertedResidual()): (\n",
      "        (INPUT:_model.features.13_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.13.conv:Sequential()): (\n",
      "          (INPUT:_model.features.13.conv_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                 dtype=oneflow.float32))\n",
      "          (MODULE:model.features.13.conv.0:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.13.conv.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.13.conv.0.0:Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "              (INPUT:_model.features.13.conv.0.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.13.conv.0.0.weight:tensor(..., device='cuda:0', size=(576, 96, 1, 1), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.13.conv.0.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.13.conv.0.1:BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.13.conv.0.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.13.conv.0.1.weight:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.13.conv.0.1.bias:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.13.conv.0.1.running_mean:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.13.conv.0.1.running_var:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.13.conv.0.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.13.conv.0.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.13.conv.0.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.13.conv.0.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.13.conv.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.13.conv.1:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.13.conv.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.13.conv.1.0:Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)): (\n",
      "              (INPUT:_model.features.13.conv.1.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.13.conv.1.0.weight:tensor(..., device='cuda:0', size=(576, 1, 3, 3), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.13.conv.1.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.13.conv.1.1:BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.13.conv.1.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.13.conv.1.1.weight:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.13.conv.1.1.bias:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.13.conv.1.1.running_mean:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.13.conv.1.1.running_var:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.13.conv.1.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.13.conv.1.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.13.conv.1.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.13.conv.1.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.13.conv.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.13.conv.2:Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "            (INPUT:_model.features.13.conv.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.13.conv.2.weight:tensor(..., device='cuda:0', size=(96, 576, 1, 1), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (OUTPUT:_model.features.13.conv.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.13.conv.3:BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "            (INPUT:_model.features.13.conv.3_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.13.conv.3.weight:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (PARAMETER:model.features.13.conv.3.bias:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (BUFFER:model.features.13.conv.3.running_mean:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32)): ()\n",
      "            (BUFFER:model.features.13.conv.3.running_var:tensor(..., device='cuda:0', size=(96,), dtype=oneflow.float32)): ()\n",
      "            (OUTPUT:_model.features.13.conv.3_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (OUTPUT:_model.features.13.conv_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.13_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.features.14:InvertedResidual()): (\n",
      "        (INPUT:_model.features.14_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.14.conv:Sequential()): (\n",
      "          (INPUT:_model.features.14.conv_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                 dtype=oneflow.float32))\n",
      "          (MODULE:model.features.14.conv.0:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.14.conv.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.14.conv.0.0:Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "              (INPUT:_model.features.14.conv.0.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 96, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.14.conv.0.0.weight:tensor(..., device='cuda:0', size=(576, 96, 1, 1), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.14.conv.0.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.14.conv.0.1:BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.14.conv.0.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.14.conv.0.1.weight:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.14.conv.0.1.bias:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.14.conv.0.1.running_mean:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.14.conv.0.1.running_var:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.14.conv.0.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.14.conv.0.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.14.conv.0.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.14.conv.0.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.14.conv.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.14.conv.1:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.14.conv.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.14.conv.1.0:Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)): (\n",
      "              (INPUT:_model.features.14.conv.1.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 2, 2),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.14.conv.1.0.weight:tensor(..., device='cuda:0', size=(576, 1, 3, 3), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.14.conv.1.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.14.conv.1.1:BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.14.conv.1.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.14.conv.1.1.weight:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.14.conv.1.1.bias:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.14.conv.1.1.running_mean:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.14.conv.1.1.running_var:tensor(..., device='cuda:0', size=(576,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.14.conv.1.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.14.conv.1.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.14.conv.1.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.14.conv.1.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.14.conv.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.14.conv.2:Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "            (INPUT:_model.features.14.conv.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 576, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.14.conv.2.weight:tensor(..., device='cuda:0', size=(160, 576, 1, 1), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (OUTPUT:_model.features.14.conv.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.14.conv.3:BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "            (INPUT:_model.features.14.conv.3_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.14.conv.3.weight:tensor(..., device='cuda:0', size=(160,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (PARAMETER:model.features.14.conv.3.bias:tensor(..., device='cuda:0', size=(160,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (BUFFER:model.features.14.conv.3.running_mean:tensor(..., device='cuda:0', size=(160,), dtype=oneflow.float32)): ()\n",
      "            (BUFFER:model.features.14.conv.3.running_var:tensor(..., device='cuda:0', size=(160,), dtype=oneflow.float32)): ()\n",
      "            (OUTPUT:_model.features.14.conv.3_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (OUTPUT:_model.features.14.conv_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.14_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.features.15:InvertedResidual()): (\n",
      "        (INPUT:_model.features.15_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.15.conv:Sequential()): (\n",
      "          (INPUT:_model.features.15.conv_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                 dtype=oneflow.float32))\n",
      "          (MODULE:model.features.15.conv.0:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.15.conv.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.15.conv.0.0:Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "              (INPUT:_model.features.15.conv.0.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.15.conv.0.0.weight:tensor(..., device='cuda:0', size=(960, 160, 1, 1), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.15.conv.0.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.15.conv.0.1:BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.15.conv.0.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.15.conv.0.1.weight:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.15.conv.0.1.bias:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.15.conv.0.1.running_mean:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.15.conv.0.1.running_var:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.15.conv.0.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.15.conv.0.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.15.conv.0.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.15.conv.0.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.15.conv.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.15.conv.1:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.15.conv.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.15.conv.1.0:Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)): (\n",
      "              (INPUT:_model.features.15.conv.1.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.15.conv.1.0.weight:tensor(..., device='cuda:0', size=(960, 1, 3, 3), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.15.conv.1.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.15.conv.1.1:BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.15.conv.1.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.15.conv.1.1.weight:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.15.conv.1.1.bias:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.15.conv.1.1.running_mean:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.15.conv.1.1.running_var:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.15.conv.1.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.15.conv.1.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.15.conv.1.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.15.conv.1.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.15.conv.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.15.conv.2:Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "            (INPUT:_model.features.15.conv.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.15.conv.2.weight:tensor(..., device='cuda:0', size=(160, 960, 1, 1), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (OUTPUT:_model.features.15.conv.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.15.conv.3:BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "            (INPUT:_model.features.15.conv.3_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.15.conv.3.weight:tensor(..., device='cuda:0', size=(160,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (PARAMETER:model.features.15.conv.3.bias:tensor(..., device='cuda:0', size=(160,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (BUFFER:model.features.15.conv.3.running_mean:tensor(..., device='cuda:0', size=(160,), dtype=oneflow.float32)): ()\n",
      "            (BUFFER:model.features.15.conv.3.running_var:tensor(..., device='cuda:0', size=(160,), dtype=oneflow.float32)): ()\n",
      "            (OUTPUT:_model.features.15.conv.3_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (OUTPUT:_model.features.15.conv_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.15_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.features.16:InvertedResidual()): (\n",
      "        (INPUT:_model.features.16_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.16.conv:Sequential()): (\n",
      "          (INPUT:_model.features.16.conv_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                 dtype=oneflow.float32))\n",
      "          (MODULE:model.features.16.conv.0:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.16.conv.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.16.conv.0.0:Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "              (INPUT:_model.features.16.conv.0.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.16.conv.0.0.weight:tensor(..., device='cuda:0', size=(960, 160, 1, 1), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.16.conv.0.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.16.conv.0.1:BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.16.conv.0.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.16.conv.0.1.weight:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.16.conv.0.1.bias:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.16.conv.0.1.running_mean:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.16.conv.0.1.running_var:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.16.conv.0.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.16.conv.0.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.16.conv.0.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.16.conv.0.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.16.conv.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.16.conv.1:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.16.conv.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.16.conv.1.0:Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)): (\n",
      "              (INPUT:_model.features.16.conv.1.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.16.conv.1.0.weight:tensor(..., device='cuda:0', size=(960, 1, 3, 3), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.16.conv.1.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.16.conv.1.1:BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.16.conv.1.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.16.conv.1.1.weight:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.16.conv.1.1.bias:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.16.conv.1.1.running_mean:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.16.conv.1.1.running_var:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.16.conv.1.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.16.conv.1.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.16.conv.1.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.16.conv.1.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.16.conv.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.16.conv.2:Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "            (INPUT:_model.features.16.conv.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.16.conv.2.weight:tensor(..., device='cuda:0', size=(160, 960, 1, 1), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (OUTPUT:_model.features.16.conv.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.16.conv.3:BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "            (INPUT:_model.features.16.conv.3_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.16.conv.3.weight:tensor(..., device='cuda:0', size=(160,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (PARAMETER:model.features.16.conv.3.bias:tensor(..., device='cuda:0', size=(160,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (BUFFER:model.features.16.conv.3.running_mean:tensor(..., device='cuda:0', size=(160,), dtype=oneflow.float32)): ()\n",
      "            (BUFFER:model.features.16.conv.3.running_var:tensor(..., device='cuda:0', size=(160,), dtype=oneflow.float32)): ()\n",
      "            (OUTPUT:_model.features.16.conv.3_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (OUTPUT:_model.features.16.conv_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.16_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.features.17:InvertedResidual()): (\n",
      "        (INPUT:_model.features.17_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.17.conv:Sequential()): (\n",
      "          (INPUT:_model.features.17.conv_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                 dtype=oneflow.float32))\n",
      "          (MODULE:model.features.17.conv.0:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.17.conv.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.17.conv.0.0:Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "              (INPUT:_model.features.17.conv.0.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 160, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.17.conv.0.0.weight:tensor(..., device='cuda:0', size=(960, 160, 1, 1), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.17.conv.0.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.17.conv.0.1:BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.17.conv.0.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.17.conv.0.1.weight:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.17.conv.0.1.bias:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.17.conv.0.1.running_mean:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.17.conv.0.1.running_var:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.17.conv.0.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.17.conv.0.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.17.conv.0.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.17.conv.0.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.17.conv.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.17.conv.1:ConvBNActivation()): (\n",
      "            (INPUT:_model.features.17.conv.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "            (MODULE:model.features.17.conv.1.0:Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)): (\n",
      "              (INPUT:_model.features.17.conv.1.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.17.conv.1.0.weight:tensor(..., device='cuda:0', size=(960, 1, 3, 3), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (OUTPUT:_model.features.17.conv.1.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.17.conv.1.1:BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "              (INPUT:_model.features.17.conv.1.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (PARAMETER:model.features.17.conv.1.1.weight:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (PARAMETER:model.features.17.conv.1.1.bias:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32,\n",
      "                     requires_grad=True)): ()\n",
      "              (BUFFER:model.features.17.conv.1.1.running_mean:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32)): ()\n",
      "              (BUFFER:model.features.17.conv.1.1.running_var:tensor(..., device='cuda:0', size=(960,), dtype=oneflow.float32)): ()\n",
      "              (OUTPUT:_model.features.17.conv.1.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (MODULE:model.features.17.conv.1.2:ReLU6(inplace=True)): (\n",
      "              (INPUT:_model.features.17.conv.1.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "              (OUTPUT:_model.features.17.conv.1.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                     dtype=oneflow.float32))\n",
      "            )\n",
      "            (OUTPUT:_model.features.17.conv.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.17.conv.2:Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "            (INPUT:_model.features.17.conv.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 960, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.17.conv.2.weight:tensor(..., device='cuda:0', size=(320, 960, 1, 1), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (OUTPUT:_model.features.17.conv.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 320, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (MODULE:model.features.17.conv.3:BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "            (INPUT:_model.features.17.conv.3_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 320, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "            (PARAMETER:model.features.17.conv.3.weight:tensor(..., device='cuda:0', size=(320,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (PARAMETER:model.features.17.conv.3.bias:tensor(..., device='cuda:0', size=(320,), dtype=oneflow.float32,\n",
      "                   requires_grad=True)): ()\n",
      "            (BUFFER:model.features.17.conv.3.running_mean:tensor(..., device='cuda:0', size=(320,), dtype=oneflow.float32)): ()\n",
      "            (BUFFER:model.features.17.conv.3.running_var:tensor(..., device='cuda:0', size=(320,), dtype=oneflow.float32)): ()\n",
      "            (OUTPUT:_model.features.17.conv.3_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 320, 1, 1),\n",
      "                   dtype=oneflow.float32))\n",
      "          )\n",
      "          (OUTPUT:_model.features.17.conv_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 320, 1, 1),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.17_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 320, 1, 1),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.features.18:ConvBNActivation()): (\n",
      "        (INPUT:_model.features.18_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 320, 1, 1),\n",
      "               dtype=oneflow.float32))\n",
      "        (MODULE:model.features.18.0:Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)): (\n",
      "          (INPUT:_model.features.18.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 320, 1, 1),\n",
      "                 dtype=oneflow.float32))\n",
      "          (PARAMETER:model.features.18.0.weight:tensor(..., device='cuda:0', size=(1280, 320, 1, 1), dtype=oneflow.float32,\n",
      "                 requires_grad=True)): ()\n",
      "          (OUTPUT:_model.features.18.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 1280, 1, 1),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (MODULE:model.features.18.1:BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)): (\n",
      "          (INPUT:_model.features.18.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 1280, 1, 1),\n",
      "                 dtype=oneflow.float32))\n",
      "          (PARAMETER:model.features.18.1.weight:tensor(..., device='cuda:0', size=(1280,), dtype=oneflow.float32,\n",
      "                 requires_grad=True)): ()\n",
      "          (PARAMETER:model.features.18.1.bias:tensor(..., device='cuda:0', size=(1280,), dtype=oneflow.float32,\n",
      "                 requires_grad=True)): ()\n",
      "          (BUFFER:model.features.18.1.running_mean:tensor(..., device='cuda:0', size=(1280,), dtype=oneflow.float32)): ()\n",
      "          (BUFFER:model.features.18.1.running_var:tensor(..., device='cuda:0', size=(1280,), dtype=oneflow.float32)): ()\n",
      "          (OUTPUT:_model.features.18.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 1280, 1, 1),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (MODULE:model.features.18.2:ReLU6(inplace=True)): (\n",
      "          (INPUT:_model.features.18.2_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 1280, 1, 1),\n",
      "                 dtype=oneflow.float32))\n",
      "          (OUTPUT:_model.features.18.2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 1280, 1, 1),\n",
      "                 dtype=oneflow.float32))\n",
      "        )\n",
      "        (OUTPUT:_model.features.18_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 1280, 1, 1),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (OUTPUT:_model.features_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 1280, 1, 1),\n",
      "             dtype=oneflow.float32))\n",
      "    )\n",
      "    (MODULE:model.adaptive_avg_pool2d:AdaptiveAvgPool2d()): (\n",
      "      (INPUT:_model.adaptive_avg_pool2d_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 1280, 1, 1),\n",
      "             dtype=oneflow.float32))\n",
      "      (OUTPUT:_model.adaptive_avg_pool2d_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 1280, 1, 1),\n",
      "             dtype=oneflow.float32))\n",
      "    )\n",
      "    (MODULE:model.classifier:Sequential()): (\n",
      "      (INPUT:_model.classifier_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 1280),\n",
      "             dtype=oneflow.float32))\n",
      "      (MODULE:model.classifier.0:Dropout(p=0.2, inplace=False)): (\n",
      "        (INPUT:_model.classifier.0_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 1280),\n",
      "               dtype=oneflow.float32))\n",
      "        (OUTPUT:_model.classifier.0_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 1280),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (MODULE:model.classifier.1:Linear(in_features=1280, out_features=1000, bias=True)): (\n",
      "        (INPUT:_model.classifier.1_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 1280),\n",
      "               dtype=oneflow.float32))\n",
      "        (PARAMETER:model.classifier.1.weight:tensor(..., device='cuda:0', size=(1000, 1280), dtype=oneflow.float32,\n",
      "               requires_grad=True)): ()\n",
      "        (PARAMETER:model.classifier.1.bias:tensor(..., device='cuda:0', size=(1000,), dtype=oneflow.float32,\n",
      "               requires_grad=True)): ()\n",
      "        (OUTPUT:_model.classifier.1_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 1000),\n",
      "               dtype=oneflow.float32))\n",
      "      )\n",
      "      (OUTPUT:_model.classifier_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 1000),\n",
      "             dtype=oneflow.float32))\n",
      "    )\n",
      "    (MODULE:model.classifer:Sequential()): (\n",
      "      (MODULE:model.classifer.0:Dropout(p=0.2, inplace=False)): ()\n",
      "      (MODULE:model.classifer.1:Linear(in_features=1280, out_features=10, bias=True)): (\n",
      "        (PARAMETER:model.classifer.1.weight:tensor(..., size=(10, 1280), dtype=oneflow.float32, requires_grad=True)): ()\n",
      "        (PARAMETER:model.classifer.1.bias:tensor(..., size=(10,), dtype=oneflow.float32, requires_grad=True)): ()\n",
      "      )\n",
      "    )\n",
      "    (OUTPUT:_model_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 1000),\n",
      "           dtype=oneflow.float32))\n",
      "  )\n",
      "  (MODULE:loss_fn:CrossEntropyLoss()): (\n",
      "    (INPUT:_loss_fn_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(64, 1000),\n",
      "           dtype=oneflow.float32))\n",
      "    (INPUT:_loss_fn_input.0.1_3:tensor(..., device='cuda:0', is_lazy='True', size=(64,), dtype=oneflow.int64))\n",
      "    (OUTPUT:_loss_fn_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(), dtype=oneflow.float32))\n",
      "  )\n",
      "  (OUTPUT:_GraphMobileNetV2_2_output.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(), dtype=oneflow.float32))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(graph_mobile_net_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据 Graph 对象是否 **已经被调用过**，输出的效果略有不同：\n",
    "\n",
    "如果 Graph 对象调用前 `print`，输出的是网络结构的信息。\n",
    "\n",
    "以上 `graph_mobile_net_v2` 调用前 `print` 效果：\n",
    "\n",
    "```text\n",
    "(GRAPH:GraphMobileNetV2_0:GraphMobileNetV2): (\n",
    "  (CONFIG:config:GraphConfig(training=True, ))\n",
    "  (MODULE:model:MobileNetV2()): (\n",
    "    (MODULE:model.features:Sequential()): (\n",
    "      (MODULE:model.features.0:ConvBNActivation()): (\n",
    "        (MODULE:model.features.0.0:Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)): (\n",
    "          (PARAMETER:model.features.0.0.weight:tensor(..., device='cuda:0', size=(32, 3, 3, 3), dtype=oneflow.float32,\n",
    "                 requires_grad=True)): ()\n",
    "        )\n",
    "    ...\n",
    "    (MODULE:model.classifer:Sequential()): (\n",
    "      (MODULE:model.classifer.0:Dropout(p=0.2, inplace=False)): ()\n",
    "      (MODULE:model.classifer.1:Linear(in_features=1280, out_features=10, bias=True)): (\n",
    "        (PARAMETER:model.classifer.1.weight:tensor(..., size=(10, 1280), dtype=oneflow.float32, requires_grad=True)): ()\n",
    "        (PARAMETER:model.classifer.1.bias:tensor(..., size=(10,), dtype=oneflow.float32, requires_grad=True)): ()\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (MODULE:loss_fn:CrossEntropyLoss()): ()\n",
    ")\n",
    "```\n",
    "\n",
    "在上面的调试信息中，表示基于 Sequential 模型，网络中自定义了 `ConvBNActivation` (对应 `MBConv` 模块)、卷积层(包括详细的 `channel`、`kernel_size` 和 `stride` 等参数信息)、`Dropout`  和全连接层等结构。\n",
    "\n",
    "如果是 Graph 对象调用后 `print`，除了网络的结构信息外，还会打印输入输出张量的信息，有如下类似效果：\n",
    "\n",
    "```text\n",
    "(GRAPH:GraphMobileNetV2_0:GraphMobileNetV2): (\n",
    "  (CONFIG:config:GraphConfig(training=True, ))\n",
    "  (INPUT:_GraphMobileNetV2_0-input_0:tensor(..., device='cuda:0', size=(64, 3, 32, 32), dtype=oneflow.float32))\n",
    "  (INPUT:_GraphMobileNetV2_0-input_1:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.int64))\n",
    "  (MODULE:model:MobileNetV2()): (\n",
    "    (INPUT:_model-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(64, 3, 32, 32),\n",
    "           dtype=oneflow.float32))\n",
    "    (MODULE:model.features:Sequential()): (\n",
    "      (INPUT:_model.features-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(64, 3, 32, 32),\n",
    "             dtype=oneflow.float32))\n",
    "      (MODULE:model.features.0:ConvBNActivation()): (\n",
    "        (INPUT:_model.features.0-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(64, 3, 32, 32),\n",
    "               dtype=oneflow.float32))\n",
    "        (MODULE:model.features.0.0:Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)): (\n",
    "          (INPUT:_model.features.0.0-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(64, 3, 32, 32),\n",
    "                 dtype=oneflow.float32))\n",
    "          (PARAMETER:model.features.0.0.weight:tensor(..., device='cuda:0', size=(32, 3, 3, 3), dtype=oneflow.float32,\n",
    "                 requires_grad=True)): ()\n",
    "          (OUTPUT:_model.features.0.0-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16),\n",
    "                 dtype=oneflow.float32))\n",
    "        )\n",
    "    ...\n",
    "```\n",
    "\n",
    "**第二种** 方式是调用 Graph 对象的 [debug](https://start.oneflow.org/oneflow-api-cn/graph.html#oneflow.nn.Graph.debug) 方法，就开启了 Graph 的调试模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_mobile_net_v2.debug(v_level=1) # v_level 参数默认值为 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以简写为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_mobile_net_v2.debug(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneFlow 在编译生成计算图的过程中会打印调试信息，比如，将上面例子代码中 `graph_mobile_net_v2.debug()` 的注释去掉，将在控制台上输出如下输出：\n",
    "\n",
    "```text\n",
    "(GRAPH:GraphMobileNetV2_0:GraphMobileNetV2) end building graph.\n",
    "(GRAPH:GraphMobileNetV2_0:GraphMobileNetV2) start compiling plan and init graph runtime.\n",
    "(GRAPH:GraphMobileNetV2_0:GraphMobileNetV2) end compiling plan and init graph rumtime.\n",
    "```\n",
    "\n",
    "使用 `debug` 的好处在于，调试信息是 **边构图、边输出** 的，这样如果构图过程中发生错误，容易发现构图时的问题。\n",
    "\n",
    "当前可以使用 `v_level` 选择详细调试信息级别，默认级别为 0，最大级别为 3。\n",
    "\n",
    "- `v_level=0` 时，只输出最基础的警告和构图阶段信息，如构图时间。\n",
    "- `v_level=1` 时，将额外打印每个 `nn.Module` 的构图信息，具体内容在下面的表格中介绍。\n",
    "- `v_level=2` 时，在构图阶段，将额外打印每个 Op 的创建信息，包括名称、输入内容、设备和 SBP 信息等。\n",
    "- `v_level=3` 时，将额外打印每个 Op 更详细的信息，如与代码位置有关的信息，方便定位代码问题。\n",
    "\n",
    "此外，为了开发者对 Graph 对象下的类型有更清晰的认知，下面对 `debug` 输出的内容进行分析，基本包括 `GRAPH`、`CONFIG`、`MODULE`、`PARAMETER`、`BUFFER`、`INPUT` 和 `OUTPUT` 七个类别的标签。\n",
    "\n",
    "|      Name      |                             Info                             |                           Example                            |\n",
    "| :------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |\n",
    "|     GRAPH      |    用户所定义的 Graph 信息，依次是类型：名字：构造方法。     |        `(GRAPH:GraphMobileNetV2_0:GraphMobileNetV2)`         |\n",
    "|     CONFIG     | Graph 的配置信息。如是否处于训练模式，`training=True` 表示 Graph 处于训练模式，如果在 Graph 的预测模式，则对应 `training=False`。 |        `(CONFIG:config:GraphConfig(training=True, )`         |\n",
    "|     MODULE     | 对应 `nn.Module` ，MODULE 可以在 Graph 标签下层，同时，多个 MODULE 之间也存在层级关系。 | `(MODULE:model:MobileNetV2())`，其中，`MobileNetV2` 为用户复用 Eager 模式下的 Module 类名。 |\n",
    "|   PARAMETER    | 给出了更清晰的 weight 和 bias 信息。此外，在构图时，tensor 的数据内容不太重要，所以只展示了 tensor 的元信息，这些信息对构建网络更为重要。 | `(PARAMETER:model.features.0.1.weight:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32, requires_grad=True))` |\n",
    "|     BUFFER     |                在训练时产生的统计特性等内容，如 running_mean 和   running_var。                | `(BUFFER:model.features.0.1.running_mean:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32))` |\n",
    "| INPUT & OUPTUT |                   表示输入输出的张量信息。                   | `(INPUT:_model_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(16, 3, 32, 32), dtype=oneflow.float32))` |\n",
    "\n",
    "除了以上介绍的方法外，训练过程中获取参数的梯度、获取 learning rate 等功能，也正在开发中，即将上线。\n",
    "\n",
    "\n",
    "\n",
    "### Graph 模型的保存与加载\n",
    "\n",
    "在训练 Graph 模型时，常常需要将已经训练了一段时间的模型的参数以及其他诸如优化器参数等状态进行保存，方便中断后恢复训练。\n",
    "\n",
    "Graph 模型对象具有和 Module 类似的 `state_dict` 和 `load_state_dict` 接口，配合 [save](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.save#oneflow.save) 和 [load](https://oneflow.readthedocs.io/en/master/oneflow.html?highlight=oneflow.load#oneflow.load) 就可以实现保存与加载 Graph 模型。这与之前在 [模型的保存与加载](https://oneflow.cloud/drill/#/project/public/code?id=1ce7600d419d1a609678c547cf1a9856) 中介绍的 Eager 模式下是类似的。和 Eager 略有不同的是，在训练过程中调用 Graph 的 `state_dict` 时，除了会得到内部 Module 各层的参数，也会得到训练迭代数、优化器参数等其他状态，以便之后恢复训练。\n",
    "\n",
    "例如，希望在以上训练 `graph_mobile_net_v2` 的过程中，每经过 1 个 epoch 将模型最新的状态保存一次，那么可以添加以下代码：\n",
    "\n",
    "假设我们想要保存在当前目录下的 \"GraphMobileNetV2\" 中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_SAVE_DIR = \"./GraphMobileNetV2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在每个 epoch 训练完成处插入以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(CHECKPOINT_SAVE_DIR)  # 清理上一次的状态\n",
    "flow.save(graph_mobile_net_v2.state_dict(), CHECKPOINT_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意**不能** 用以下方式保存。因为 Graph 在初始化时，会对成员做处理，所以 `graph_mobile_net_v2.model` 其实已经不再是 Module 类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.save(graph_mobile_net_v2.model.state_dict(), CHECKPOINT_SAVE_DIR)  # 会报错"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，如果需要恢复之前保存的状态，可以使用以下方法：\n",
    "```python\n",
    "state_dict = flow.load(CHECKPOINT_SAVE_DIR)\n",
    "graph_mobile_net_v2.load_state_dict(state_dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph 与部署\n",
    "\n",
    "nn.Graph 支持同时保存模型参数和计算图，可以很方便的支持模型部署。\n",
    "\n",
    "如果有模型部署的需求，那么应该通过 `oneflow.save` 接口，将 `Graph` 对象导出为部署需要的格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_DIR=\"./mobile_net_v2_model\"\n",
    "\n",
    "import os\n",
    "if not os.path.exists(MODEL_SAVE_DIR):\n",
    "    os.makedirs(MODEL_SAVE_DIR)\n",
    "\n",
    "flow.save(graph_mobile_net_v2, MODEL_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意和上一节的区别。 `save` 接口既支持保存 state_dict，也支持保存 Graph 对象。当保存 Graph 对象时，模型参数和计算图将被同时保存，以与模型结构定义代码解耦。\n",
    "\n",
    "这样，`./mobile_net_v2_model` 目录下会同时保存部署所需的模型参数和计算图。详细的部署流程可以参阅 [模型部署](https://docs.oneflow.org/v0.7.0/cookies/serving.html) 一文。\n",
    "\n",
    "因为部署所需的格式，必需通过 Graph 对象导出。所以，如果是 Eager 模式下训练得到的模型（即 `nn.Module` 对象），需要用 `Graph` 将 Module 封装后再导出。\n",
    "\n",
    "下面我们以 flowvision 仓库中的 `neural_style_transfer` 为例子，展示如何封装并导出 `nn.Module` 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oneflow as flow\n",
    "import oneflow.nn as nn\n",
    "from flowvision.models.neural_style_transfer.stylenet import neural_style_transfer\n",
    "\n",
    "\n",
    "class MyGraph(nn.Graph):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def build(self, *input):\n",
    "        return self.model(*input)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fake_image = flow.ones((1, 3, 256, 256))\n",
    "    model = neural_style_transfer(pretrained=True, progress=True)\n",
    "    model.eval()\n",
    "    graph = MyGraph(model)\n",
    "    out = graph(fake_image)\n",
    "    \n",
    "    \n",
    "    MODEL_SAVE_DIR=\"./neural_style_transfer_model\"\n",
    "    import os\n",
    "    if not os.path.exists(MODEL_SAVE_DIR):\n",
    "        os.makedirs(MODEL_SAVE_DIR)\n",
    "    flow.save(graph, MODEL_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码几处的关键代码：\n",
    "\n",
    "- 定义了一个 `MyGraph` 类，将 `nn.Module` 对象简单地封装一层（`return self.model(*input)`），作用仅仅是将 `nn.Module` 转为 `Graph` 对象。\n",
    "- 实例化得到 `Graph` 对象（`graph = MyGraph(model)`）\n",
    "- 调用一次 `Graph` 实例化对象（`out = graph(fake_image)`）。它内部的机理是利用 “假数据” 在模型中流动一遍（即 tracing 机制）来建立计算图。\n",
    "- 导出部署所需的模型：`flow.save(graph, \"1/model\")`\n",
    "\n",
    "## 扩展阅读：动态图与静态图\n",
    "\n",
    "用户定义的神经网络，都会被深度学习框架转为计算图，如 [自动求梯度](https://oneflow.cloud/drill/#/project/public/code?id=d05f6d1ec8e587456389b95270d3b7ac) 中的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m oneflow --doctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_pred, y):\n",
    "    return flow.sum(1/2*(y_pred-y)**2)\n",
    "\n",
    "x = flow.ones(1, 5)  # 输入\n",
    "w = flow.randn(5, 3, requires_grad=True)\n",
    "b = flow.randn(1, 3, requires_grad=True)\n",
    "z = flow.matmul(x, w) + b\n",
    "\n",
    "y = flow.zeros(1, 3)  # label\n",
    "l = loss(z,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对应的计算图为：\n",
    "\n",
    "![计算图](./imgs/compute_graph.png)\n",
    "\n",
    "**动态图（Dynamic Graph）**\n",
    "\n",
    "动态图的特点在于，它是一边执行代码，一边完成计算图的构建的。\n",
    "以上代码和构图关系可看下图（注意：下图对简单的语句做了合并）\n",
    "\n",
    "![](./imgs/dynamic_graph.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为动态图是一边执行一边构图，所以很灵活，可以随时修改图的结构，运行一行代码就能得到一行的结果，易于调试。但是因为深度学习框架无法获取完整的图信息（随时可以改变、永远不能认为构图已经完成），因此无法进行充分的全局优化，在性能上会相对欠缺。\n",
    "\n",
    "**静态图（Static Graph）**\n",
    "\n",
    "与动态图不同，静态图先定义完整的计算图。即需要用户先声明所有计算节点后，框架才开始进行计算。这可以理解为在用户代码与最终运行的计算图之间，框架起到了编译器的作用。\n",
    "\n",
    "![static graph](./imgs/static_graph.png)\n",
    "\n",
    "以 OneFlow 框架为例，用户的代码会被先转换为完整的计算图，然后再由 OneFlow Runtime 模块运行。\n",
    "\n",
    "静态图这种先获取完整网络，再编译运行的方式，使得它可以做很多动态图做不到的优化，因此性能上更有优势。并且编译完成后的计算图，也更容易跨平台部署。\n",
    "\n",
    "不过，在静态图中真正的计算发生时，已经与用户的代码没有直接关系了，因此静态图的调试较不方便。\n",
    "\n",
    "两种方式对比总结如下：\n",
    "\n",
    "|              | 动态图 | 静态图   |\n",
    "| ------------ | ------------------------------------- | ------------------------ |\n",
    "| 计算方式 | Eager 模式                             | Graph 模式                 |\n",
    "| 优点     | 代码编写灵活，易于调试                | 性能好，易于优化和部署   |\n",
    "| 缺点     | 性能及可移植性差                      | 不易调试                 |\n",
    "\n",
    "OneFlow 提供的 Eager 模式，与 PyTorch 对齐，让熟悉 PyTorch 的用户可以零成本直接上手。\n",
    "OneFlow 提供的 Graph 模式，也基于面向对象的编程风格，让熟悉 Eager 开发的用户，只需要改很少量的代码，就可以使用高效率的静态图。\n",
    "\n",
    "## 相关链接\n",
    "\n",
    "OneFlow Eager模式下的神经网络搭建：[搭建神经网络](https://oneflow.cloud/drill/#/project/public/code?id=7e351fbd41fef653f26078e91915e76d)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
